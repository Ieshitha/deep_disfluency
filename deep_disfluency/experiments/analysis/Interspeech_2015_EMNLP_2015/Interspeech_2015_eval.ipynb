{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "# add the deep_disfluency module to the path\n",
    "sys.path.append(\"../../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deep_disfluency.evaluation.disf_evaluation import incremental_output_disfluency_eval_from_file\n",
    "from deep_disfluency.evaluation.disf_evaluation import final_output_disfluency_eval_from_file\n",
    "from deep_disfluency.evaluation.eval_utils import get_tag_data_from_corpus_file\n",
    "from deep_disfluency.evaluation.eval_utils import rename_all_repairs_in_line_with_index\n",
    "from deep_disfluency.evaluation.eval_utils import sort_into_dialogue_speakers\n",
    "from deep_disfluency.evaluation.results_utils import convert_to_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the locations of all needed files\n",
    "# Assume we have the incremental output\n",
    "experiment_dir = \"../../../experiments\"\n",
    "\n",
    "partial_words = False  # No partial words in these experiments, removed\n",
    "if partial_words:\n",
    "    partial = '_partial'\n",
    "else:\n",
    "    partial = ''\n",
    "#the evaluation files (as text files)\n",
    "disf_dir = \"../../../data/disfluency_detection/switchboard\"\n",
    "disfluency_files = [\n",
    "                    disf_dir + \"/swbd_disf_heldout{}_data_timings.csv\".format(partial),\n",
    "                    disf_dir + \"/swbd_disf_test{}_data_timings.csv\".format(partial)\n",
    "                   ]\n",
    "    \n",
    "\n",
    "allsystemsfinal = [\"021/epoch_40\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "021/epoch_40\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing final output to file ../../../experiments/021/epoch_40/swbd_disf_heldout_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "t_t_detection_<e_word 0.173286991063\n",
      "delayed_acc_<rm_4_word None\n",
      "t_t_detection_<rms_word 2.51349380015\n",
      "delayed_acc_<rm_1_word None\n",
      "edit_overhead_rel_word 5.94697924544\n",
      "t_t_detection_final_t/>_word None\n",
      "delayed_acc_<rm_5_word None\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word nan\n",
      "t_t_detection_<rps_word 1.20744356315\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n",
      "writing final output to file ../../../experiments/021/epoch_40/swbd_disf_test_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "t_t_detection_<e_word 0.149568034557\n",
      "delayed_acc_<rm_4_word None\n",
      "t_t_detection_<rms_word 2.48856358646\n",
      "delayed_acc_<rm_1_word None\n",
      "edit_overhead_rel_word 4.89438433797\n",
      "t_t_detection_final_t/>_word None\n",
      "delayed_acc_<rm_5_word None\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word nan\n",
      "t_t_detection_<rps_word 1.12246553122\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n"
     ]
    }
   ],
   "source": [
    "# create final output files for the final output evaluation (and do incremental evaluation first:\n",
    "DO_INCREMENTAL_EVAL = True\n",
    "if DO_INCREMENTAL_EVAL:\n",
    "    all_incremental_results = {}\n",
    "    all_incremental_error_dicts = {}\n",
    "    for system in allsystemsfinal:\n",
    "        print system\n",
    "        #if 'complex' in system: break\n",
    "        hyp_dir = experiment_dir + \"/\" + system\n",
    "        #hyp_dir = experiment_dir\n",
    "        for division, disf_file in zip([\"heldout\",\"test\"], disfluency_files):\n",
    "            print \"*\" * 30, division, \"*\" * 30\n",
    "            IDs, timings, words, pos_tags, labels = get_tag_data_from_corpus_file(disf_file)\n",
    "            gold_data = {} #map from the file name to the data\n",
    "            for dialogue,a,b,c,d in zip(IDs, timings, words, pos_tags, labels):\n",
    "                # if \"asr\" in division and not dialogue[:4] in good_asr: continue\n",
    "                gold_data[dialogue] = (a,b,c,d)\n",
    "            inc_filename = hyp_dir + \"/swbd_disf_{0}{1}_data_output_increco\".format(division, partial) + \".text\"\n",
    "            final_output_name = inc_filename.replace(\"_increco\", \"_final\")\n",
    "            results, error_analysis = incremental_output_disfluency_eval_from_file(\n",
    "                                                                             inc_filename,\n",
    "                                                                             gold_data,\n",
    "                                                                             utt_eval=True,\n",
    "                                                                             error_analysis=True,\n",
    "                                                                             word=True,\n",
    "                                                                             interval=False,\n",
    "                                                                             outputfilename=final_output_name)\n",
    "            for k,v in results.items():\n",
    "                print k,v\n",
    "            all_incremental_results[division + \"_\" + system] = deepcopy(results)\n",
    "            if \"heldout\" in division:\n",
    "                # only do the error analyses on the heldout data\n",
    "                all_incremental_error_dicts[division + \"_\" + system] = deepcopy(error_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>TTD$_{rms}$ (word)</th>\n",
       "      <th>TTD$_{rps}$ (word)</th>\n",
       "      <th>EO (word)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN (window length=2) (+ POS) (transcript)</td>\n",
       "      <td>2.489</td>\n",
       "      <td>1.122</td>\n",
       "      <td>4.894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        System (eval. method) TTD$_{rms}$ (word)  \\\n",
       "0  RNN (window length=2) (+ POS) (transcript)              2.489   \n",
       "\n",
       "  TTD$_{rps}$ (word) EO (word)  \n",
       "0              1.122     4.894  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = \"No incremental results here\"\n",
    "if DO_INCREMENTAL_EVAL:\n",
    "    display_results = dict()\n",
    "    display_results['RNN (window length=2) (+ POS)'] = all_incremental_results['test_021/epoch_40']\n",
    "    final = convert_to_latex(display_results, eval_level=['word'], inc=True, utt_seg=False, only_include=\n",
    "                            ['t_t_detection_<rms_word', 't_t_detection_<rps_word', 'edit_overhead_rel_word'])\n",
    "    #final = final.drop(final.columns[[-2]], axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final output evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "021/epoch_40\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= False\n",
      "word\n",
      "pearson_r_p_value_rps_number 1.66263671251e-74\n",
      "p_<rm_word 0.788956481048\n",
      "spearman_rank_p_value_rps_rate_per_word 1.36872889118e-44\n",
      "r_<rpnsub_word 0.329761904762\n",
      "pearson_r_p_value_rps_rate_per_utt 5.97040414387e-78\n",
      "r_<rm.<i.<rp_word 0.573212083847\n",
      "r_<i_word 0.625263157895\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.545054945055\n",
      "pearson_r_correl_rps_number 0.982254549874\n",
      "p_<rpnrep_word 0.863360323887\n",
      "p_<i_word 0.661469933185\n",
      "r_<e_word 0.891726251277\n",
      "p_<rpnsub_word 0.5\n",
      "r_<rm_word 0.616228070175\n",
      "p_<rms_word 0.754182754183\n",
      "p_<rps_word 0.836544074723\n",
      "r_<rpn_word 0.623248572911\n",
      "pearson_r_p_value_rps_rate_per_word 5.2924511869e-59\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0.699701492537\n",
      "f1_<e_word 0.873873873874\n",
      "r_<rps_word 0.736382322713\n",
      "f1_<rpn_word 0.682386363636\n",
      "p_<rpn_word 0.75392341494\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.654868814932\n",
      "spearman_rank_correl_rps_rate_per_utt 0.930678428846\n",
      "p_<rps_relaxed_word 0.870986573263\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.529752822704\n",
      "f1_<rm.<i.<rp_word 0.611325717104\n",
      "f1_<rps_relaxed_word 0.81552336704\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0.691976195362\n",
      "r_<rps_relaxed_word 0.766700924974\n",
      "spearman_rank_p_value_rps_number 3.35814776346e-49\n",
      "p_<rm.<rp.<i_word None\n",
      "f1_<rms_word 0.725921337876\n",
      "f1_<rpnrep_word 0.857717445953\n",
      "p_<rp_word 0.56126737795\n",
      "r_<rpnrep_word 0.852147852148\n",
      "f1_<rps_word 0.783274118612\n",
      "f1_<i_word 0.642857142857\n",
      "spearman_rank_correl_rps_rate_per_word 0.927718405428\n",
      "pearson_r_correl_rps_rate_per_utt 0.984876940034\n",
      "pearson_r_correl_rps_rate_per_word 0.96343011608\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.856722276742\n",
      "r_<rpndel_word 0\n",
      "f1_<rpnsub_word 0.397417503587\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.941956056719\n",
      "spearman_rank_p_value_rps_rate_per_utt 1.82102659455e-45\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= False\n",
      "word\n",
      "pearson_r_p_value_rps_number 1.28745644031e-53\n",
      "p_<rm_word 0.779518746503\n",
      "spearman_rank_p_value_rps_rate_per_word 1.9685239478e-37\n",
      "r_<rpnsub_word 0.256838905775\n",
      "pearson_r_p_value_rps_rate_per_utt 8.96732525312e-54\n",
      "r_<rm.<i.<rp_word 0.541071428571\n",
      "r_<i_word 0.517401392111\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.512133285042\n",
      "pearson_r_correl_rps_number 0.955191318554\n",
      "p_<rpnrep_word 0.897058823529\n",
      "p_<i_word 0.612637362637\n",
      "r_<e_word 0.87751813054\n",
      "p_<rpnsub_word 0.403341288783\n",
      "r_<rm_word 0.584312080537\n",
      "p_<rms_word 0.728244274809\n",
      "p_<rps_word 0.813669064748\n",
      "r_<rpn_word 0.581751377832\n",
      "pearson_r_p_value_rps_rate_per_word 7.78576577451e-40\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0.656572608396\n",
      "f1_<e_word 0.852668667624\n",
      "r_<rps_word 0.685454545455\n",
      "f1_<rpn_word 0.649572649573\n",
      "p_<rpn_word 0.735294117647\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.619885433715\n",
      "spearman_rank_correl_rps_rate_per_utt 0.95587233683\n",
      "p_<rps_relaxed_word 0.864028776978\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.507719928187\n",
      "f1_<rm.<i.<rp_word 0.577803203661\n",
      "f1_<rps_relaxed_word 0.790131578947\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0.66794533685\n",
      "r_<rps_relaxed_word 0.727878787879\n",
      "spearman_rank_p_value_rps_number 1.5016569762e-50\n",
      "p_<rm.<rp.<i_word None\n",
      "f1_<rms_word 0.690553745928\n",
      "f1_<rpnrep_word 0.852649970879\n",
      "p_<rp_word 0.516624040921\n",
      "r_<rpnrep_word 0.81243063263\n",
      "f1_<rps_word 0.744078947368\n",
      "f1_<i_word 0.561006289308\n",
      "spearman_rank_correl_rps_rate_per_word 0.901544859125\n",
      "pearson_r_correl_rps_rate_per_utt 0.955528180291\n",
      "pearson_r_correl_rps_rate_per_word 0.912541623741\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.829187817259\n",
      "r_<rpndel_word 0\n",
      "f1_<rpnsub_word 0.313834726091\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.948063592901\n",
      "spearman_rank_p_value_rps_rate_per_utt 6.1791709704e-54\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "all_error_dicts = {}\n",
    "for system in allsystemsfinal:\n",
    "    print system\n",
    "    #if 'complex' in system: break\n",
    "    hyp_dir = experiment_dir\n",
    "    for division, disf_file in zip([\"heldout\", \"test\"],disfluency_files):\n",
    "        #if not division == \"heldout\": continue\n",
    "        print \"*\" * 30, division, \"*\" * 30\n",
    "        IDs, timings, words, pos_tags, labels = get_tag_data_from_corpus_file(disf_file)\n",
    "        gold_data = {} #map from the file name to the data\n",
    "        for dialogue,a,b,c,d in zip(IDs, timings, words, pos_tags, labels):\n",
    "            # if \"asr\" in division and not dialogue[:4] in good_asr: continue\n",
    "            d = rename_all_repairs_in_line_with_index(list(d))\n",
    "            gold_data[dialogue] = (a,b,c,d)\n",
    "\n",
    "        #the below does just the final output evaluation, assuming a final output file, faster\n",
    "        hyp_file = hyp_dir + '/' + system + \"/\" + \"swbd_disf_{0}{1}_data_output_final.text\".format(division,\n",
    "                                                                                                        partial)\n",
    "        word = True  # world-level analyses\n",
    "        error = True # get an error analysis\n",
    "        results,speaker_rate_dict,error_analysis = final_output_disfluency_eval_from_file(\n",
    "                                                        hyp_file,\n",
    "                                                        gold_data,\n",
    "                                                        utt_eval=False,\n",
    "                                                        error_analysis=error,\n",
    "                                                        word=word,\n",
    "                                                        interval=False,\n",
    "                                                        outputfilename=None\n",
    "                                                    )\n",
    "        #the below does incremental and final output in one, also outputting the final outputs\n",
    "        #derivable from the incremental output, takes quite a while\n",
    "        for k,v in results.items():\n",
    "            print k,v\n",
    "        all_results[division + \"_\" + system] = deepcopy(results)\n",
    "        if \"heldout\" in division:\n",
    "            # only do the error analyses on the heldout data\n",
    "            all_error_dicts[division + \"_\" + system] = deepcopy(error_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heldout_021/epoch_40', 'test_021/epoch_40']\n"
     ]
    }
   ],
   "source": [
    "print all_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>$F_{rm}$ (per word)</th>\n",
       "      <th>$F_{rps}$ (per word)</th>\n",
       "      <th>$F_{e}$ (per word)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN (window length=2) (+ POS) (transcript)</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        System (eval. method) $F_{rm}$ (per word)  \\\n",
       "0  RNN (window length=2) (+ POS) (transcript)               0.668   \n",
       "\n",
       "  $F_{rps}$ (per word) $F_{e}$ (per word)  \n",
       "0                0.744              0.853  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results = dict()\n",
    "display_results['RNN (window length=2) (+ POS)'] = all_results['test_021/epoch_40']\n",
    "final = convert_to_latex(display_results, eval_level=['word'], inc=False, utt_seg=False, only_include=\n",
    "                        ['f1_<rm_word', 'f1_<rps_word', 'f1_<e_word'])\n",
    "#final = final.drop(final.columns[[-2]], axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** heldout_021/epoch_40 <rps ******************************\n",
      "type ******************************\n",
      "del & (31/101) & 0.463\\\\\n",
      "rep & (892/1001) & 0.930\\\\\n",
      "sub & (510/844) & 0.671\\\\\n",
      "0.736382322713\n",
      "len ******************************\n",
      "0 & (0/1) & 0.000\\\\\n",
      "1 & (917/1096) & 0.881\\\\\n",
      "2 & (338/467) & 0.745\\\\\n",
      "3 & (102/205) & 0.590\\\\\n",
      "4 & (44/88) & 0.543\\\\\n",
      "5 & (16/46) & 0.492\\\\\n",
      "6 & (10/26) & 0.541\\\\\n",
      "7 & (3/8) & 0.545\\\\\n",
      "8 & (2/5) & 0.571\\\\\n",
      "9 & (1/1) & 1.000\\\\\n",
      "10 & (0/1) & 0.000\\\\\n",
      "11 & (0/1) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.736382322713\n",
      "****************************** heldout_021/epoch_40 <rms ******************************\n",
      "type ******************************\n",
      "del & (0/75) & 0.000\\\\\n",
      "rep & (903/1034) & 0.884\\\\\n",
      "sub & (250/547) & 0.492\\\\\n",
      "0.696256038647\n",
      "len ******************************\n",
      "1 & (866/1065) & 0.845\\\\\n",
      "2 & (236/363) & 0.640\\\\\n",
      "3 & (51/143) & 0.382\\\\\n",
      "4 & (16/53) & 0.296\\\\\n",
      "5 & (3/29) & 0.136\\\\\n",
      "6 & (0/15) & 0.000\\\\\n",
      "7 & (0/3) & 0.000\\\\\n",
      "8 & (0/2) & 0.000\\\\\n",
      "9 & (0/1) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.699701492537\n"
     ]
    }
   ],
   "source": [
    "#Error analyses on exact match ('rms') and getting the right repair start ('rps')\n",
    "target_tags = ['<rms', '<rps']\n",
    "\n",
    "for div,all_error in all_error_dicts.items():\n",
    "    # print div, type(all_error)\n",
    "   \n",
    "    if type(all_error) == bool: continue\n",
    "    if \"test\" in div: continue\n",
    "    #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "    for tag, errors in all_error.items():\n",
    "        if tag not in target_tags:\n",
    "            continue\n",
    "        print \"*\" * 30, div, tag, \"*\" * 30\n",
    "        # print errors\n",
    "        # continue\n",
    "        #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "        error = {\"TP\" : {}, \"FP\" : {}, \"FN\": {} }\n",
    "        for k,v in errors.items():\n",
    "            #if k == \"FP\":\n",
    "            #    continue\n",
    "            # print k, len(v)\n",
    "            typedict = defaultdict(int)\n",
    "            lendict = defaultdict(int)\n",
    "            for repair in v:\n",
    "\n",
    "                #print repair.gold_context\n",
    "                onset = \"\"\n",
    "                if tag == \"<rps\" or tag == \"<rms\":\n",
    "                    \n",
    "                    \n",
    "                    for i in range(0,len(repair.gold_context)):\n",
    "                        if repair.gold_context[i] == \"+|+\":\n",
    "                            onset = repair.gold_context[i+1]\n",
    "                            break\n",
    "\n",
    "                    word = onset.split(\"|\")[0]\n",
    "                    #if k == \"FP\":\n",
    "                    #    onset = gold_onset\n",
    "                    if \"<e\" in onset and not tag == \"<e\":\n",
    "                        typedict[\"<e\"]+=1\n",
    "                    else:\n",
    "                        if word in [\"and\",\"or\",\"but\",\"so\",\"because\",\"that\",\"although\"]:\n",
    "                            typedict[\"CC\"]+=1\n",
    "                        elif word in [\"i\",\"we\",\"they\",\"im\",\"ive\",\"he\",\"she\",\"id\"]:\n",
    "                            typedict[\"subj\"]+=1\n",
    "                        elif word in [\"you\",\"the\"] or \"$\" in word:\n",
    "                            typedict[\"proper_other\"]+=1\n",
    "                        elif word in [\"yeah\",\"no\",\"okay\",\"yes\",\"right\",\"uh-huh\"]:\n",
    "                            typedict[\"ack\"]+=1\n",
    "                        elif word in [\"it\",\"its\"]:\n",
    "                            typedict[\"it\"]+=1\n",
    "                        else:\n",
    "                            typedict[word]+=1\n",
    "                \n",
    "                if tag == \"<rps\" or tag == \"<rms\": # and not k == 'FP':\n",
    "                    if k == \"TP\" and len(repair.reparandumWords) > 8:\n",
    "                        # should not be getting any over 8 words\n",
    "                        print \"** overlength repair!\"\n",
    "                        print repair\n",
    "                    lendict[len(repair.reparandumWords) + len(repair.interregnumWords)]+=1\n",
    "                    repair_type = None\n",
    "                    if repair.type:\n",
    "                        repair_type = repair.type \n",
    "                        typedict[repair_type]+=1\n",
    "\n",
    "            error[k]['len'] = deepcopy(lendict)\n",
    "            error[k]['type'] = deepcopy(typedict)\n",
    "\n",
    "                \n",
    "        for mode in ['type', 'len']:\n",
    "            #q1. THE RECALL RATES FOR VARIOUS GOLD REPAIR TYPES\n",
    "            print mode, \"*\" * 30\n",
    "            tps = error['TP'][mode]\n",
    "            fns = error['FN'][mode]\n",
    "            fps = error['FP'][mode]\n",
    "\n",
    "            total_tps = 0\n",
    "            total_fns = 0\n",
    "            total_fps = 0\n",
    "            top_n = 50\n",
    "            all_items = list(set(tps.keys() + fns.keys()))\n",
    "            # print all_items\n",
    "            for k in sorted(all_items,  reverse=False):\n",
    "                #print k, \"*\" * 30\n",
    "                if mode == 'type' and k not in [\"rep\", \"del\", \"sub\"]:\n",
    "                    continue\n",
    "                recall_total = tps[k] + fns[k]\n",
    "                recall = 0 if tps[k] == 0 else tps[k]/recall_total\n",
    "                precision_total = tps[k] + fps[k]\n",
    "                precision = 0 if tps[k] == 0 else tps[k]/precision_total\n",
    "                fscore = 0 if precision == 0 or recall == 0 else (2 * (precision * recall))/(precision + recall)\n",
    "                # print k, ':', tps[k], \"out of\", recall_total\n",
    "                #print k, ':', tps[k], \"out of\", precision_total\n",
    "                total_tps += tps[k]\n",
    "                total_fns += fns[k]\n",
    "                total_fps += fps[k]\n",
    "                print \" & \".join([str(k), \"({0}/{1})\".format(tps[k],recall_total), \n",
    "                                  '{0:.3f}'.format(fscore)]) + \"\\\\\\\\\"\n",
    "                top_n-=1\n",
    "                if top_n <= 0:\n",
    "                    break\n",
    "            print total_tps/(total_fns + total_tps)\n",
    "\n",
    "            if False:\n",
    "                #q2. ERROR TYPE SUMMARY\n",
    "                print \"*\" * 30\n",
    "                total = sum(fns.values()+tps.values())\n",
    "\n",
    "                errormass = 0\n",
    "                errortotal = 0\n",
    "                top_n = 20\n",
    "                for k,v in sorted(tps.items(),key= lambda x: x[1],reverse=True):\n",
    "                    print k,\"&\",v,\"&\",'{0:.2f}'.format(v/total)\n",
    "                    errormass +=(v/total * 100)\n",
    "                    errortotal+=v\n",
    "                    top_n-=1\n",
    "                    if top_n <= 0: break\n",
    "                print \"total &\",errortotal,\"&\",'{0:.2f}'.format(errormass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
