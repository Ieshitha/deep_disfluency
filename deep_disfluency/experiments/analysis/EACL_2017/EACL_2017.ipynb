{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computes the accuracies for the outputs from the EACL 2017 experiments on\n",
    "#joint incremental utterance segmentation and disfluency detection\n",
    "#this assumes the experiments are in simple_rnn_disf/rnn_disf_detection/experiments/\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "# from mumodo.mumodoIO import open_intervalframe_from_textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add the evaluation module functions\n",
    "sys.path.append(\"../../../../\")\n",
    "from deep_disfluency.evaluation.disf_evaluation import incremental_output_disfluency_eval_from_file\n",
    "from deep_disfluency.evaluation.disf_evaluation import final_output_disfluency_eval_from_file, ACCURACY_HEADER\n",
    "from deep_disfluency.evaluation.eval_utils import get_tag_data_from_corpus_file\n",
    "from deep_disfluency.evaluation.results_utils import convert_to_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_dir = \"../../../../deep_disfluency\"\n",
    "experiment_dir = top_dir + \"/experiments\"\n",
    "#experiment_dir = \"/home/dsg-labuser/Desktop/rnn_experiments/\"\n",
    "all_results = {}\n",
    "all_error_dicts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 16 LSTM (complex tags)\n",
      "heldout\n",
      "loading data ../../../../deep_disfluency/data/disfluency_detection/switchboard/swbd_disf_heldout_partial_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= False utt_eval= False\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= False\n",
      "word\n",
      "pearson_r_p_value_rps_number 1.0\n",
      "p_<rm_word 0.796618597713\n",
      "r_<rpnsub_word 0.259962049336\n",
      "pearson_r_p_value_rps_rate_per_word 1.0\n",
      "delayed_acc_<rm_4_word None\n",
      "pearson_r_p_value_rps_rate_per_utt 1.0\n",
      "delayed_acc_<rm_3_word None\n",
      "r_<rm.<i.<rp_word 0.497004319353\n",
      "r_<i_word 0.537373737374\n",
      "f1_<rpndel_word 0.013986013986\n",
      "f1_<rp_word 0.510057039928\n",
      "pearson_r_correl_rps_number nan\n",
      "delayed_acc_<rm_5_word None\n",
      "r_<rps_relaxed_word 0.738148984199\n",
      "p_<i_word 0.775510204082\n",
      "r_<e_word 0.855685874554\n",
      "processing_overhead_word None\n",
      "edit_overhead_rel_<rm None\n",
      "r_<rm_word 0.519623743107\n",
      "p_<rms_word 0.761935905821\n",
      "p_<rps_word 0.835144927536\n",
      "r_<rpn_word 0.547238703788\n",
      "p_<rpnrep_word 0.89109947644\n",
      "p_<rpndel_word 0.0909090909091\n",
      "r_<rms_word 0.601756198347\n",
      "f1_<e_word 0.894217958966\n",
      "t_t_detection_<e_word 0.140950347037\n",
      "r_<rps_word 0.624379232506\n",
      "t_t_detection_<rms_word 2.43818466354\n",
      "p_<rpn_word 0.812330623306\n",
      "t_t_detection_<rps_word 1.17586649551\n",
      "p_<rpnsub_word 0.504604051565\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.65848255492\n",
      "r_<rpndel_word 0.00757575757576\n",
      "p_<rps_relaxed_word 0.98731884058\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.472075576549\n",
      "f1_<rm.<i.<rp_word 0.566460219152\n",
      "delayed_acc_<rm_mean_word None\n",
      "f1_<rps_relaxed_word 0.844742960475\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0.628975265018\n",
      "edit_overhead_rel_word 5.30953174471\n",
      "f1_<rpn_word 0.653940550859\n",
      "f1_<rms_word 0.672438672439\n",
      "f1_<rpnrep_word 0.860900354072\n",
      "f1_<rpnsub_word 0.343143393863\n",
      "delayed_acc_<rm_1_word None\n",
      "f1_<rps_word 0.714544045466\n",
      "f1_<i_word 0.634844868735\n",
      "pearson_r_correl_rps_rate_per_utt nan\n",
      "pearson_r_correl_rps_rate_per_word nan\n",
      "delayed_acc_<rm_2_word None\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.936383928571\n",
      "r_<rpnrep_word 0.832681017613\n",
      "p_<rm.<rp.<i_word None\n",
      "p_<rp_word 0.554684949396\n",
      "r_<e_relaxed_word 0\n",
      "delayed_acc_<rm_6_word None\n"
     ]
    }
   ],
   "source": [
    "#the evaluation files (as text files)\n",
    "disf_dir = top_dir + \"/data/disfluency_detection/switchboard\"\n",
    "disfluency_files = [\n",
    "                    disf_dir+\"/swbd_disf_heldout_partial_data_timings.csv\",\n",
    "                    disf_dir+\"/swbd_disf_heldout_partial_data_timings.csv\",\n",
    "                    disf_dir+\"/swbd_disf_test_partial_data_timings.csv\",\n",
    "                    disf_dir+\"/swbd_disf_test_partial_data_timings.csv\"\n",
    "                    ]\n",
    "    \n",
    "dialogue_speakers = []\n",
    "good_asr_heldout = [line.strip(\"\\n\") for line in open(top_dir + \\\n",
    "        \"/data/disfluency_detection/swda_divisions_disfluency_detection/swbd_disf_heldout_ASR_good_ranges.text\")]\n",
    "good_asr_test = [line.strip(\"\\n\") for line in open(top_dir + \\\n",
    "            \"/data/disfluency_detection/swda_divisions_disfluency_detection/swbd_disf_test_ASR_good_ranges.text\")]\n",
    "\n",
    "#Need comparable stopping criterion\n",
    "#1. Weighted F-score all tags  or   loss all tags - have this for all\n",
    "#2. Unweighted F-score all tags would need to recompute for all / per-class loss - would need to recompute for 33\n",
    "#3. Complex rps + tto combined metric as explored - would need to recompute for all, though is closer\n",
    "#allsystemsfinal = [(33,45,'RNN'),(35,6,'LSTM'),\\\n",
    "#                   (36,15,'LSTM (complex tags) (timing)'),\\\n",
    "#                   (34,37,'RNN (complex tags) (timing)')]\n",
    "allsystemsfinal = [\n",
    "                   #(33,45,'RNN (timing)'),\n",
    "                  #(35,6,'LSTM (timing)'),\\\n",
    "                  # (37,6,'LSTM (disf only) (timing)'),\\\n",
    "                  # (38,8,'LSTM (TTO only) (timing)'),\\\n",
    "                   #(21,40,'LSTM (complex tags)')\n",
    "                  (41,16,'LSTM (complex tags)')\n",
    "                   ]\n",
    "\n",
    "for exp,e,system in allsystemsfinal:\n",
    "    print exp,e,system\n",
    "    #if 'complex' in system: break\n",
    "    hyp_dir = experiment_dir + \"/0{0}/\".format(exp)\n",
    "    for division, disf_file in zip([\"heldout\",\n",
    "                                   # \"heldout_asr\",\n",
    "                                   # \"test\",\n",
    "                                   # \"test_asr\"\n",
    "                                   ],disfluency_files):\n",
    "        #if not division == \"heldout\": continue\n",
    "        print division\n",
    "        #dialogue_speakers.extend(sort_into_dialogue_speakers(IDs,mappings,utts, pos_tags, labels))\n",
    "        if \"heldout\" in division:\n",
    "            good_asr = good_asr_heldout\n",
    "        else:\n",
    "            good_asr = good_asr_test\n",
    "        gold_data = {} #map from the file name to the data\n",
    "        \n",
    "        IDs, timings, words, pos_tags, labels = get_tag_data_from_corpus_file(disf_file)\n",
    "        for dialogue,a,b,c,d in zip(IDs, timings, words, pos_tags, labels):\n",
    "            if \"asr\" in division and not dialogue[:4] in good_asr: continue\n",
    "            gold_data[dialogue] = (a,b,c,d)\n",
    "        if \"asr\" in division:\n",
    "            error = False\n",
    "            word = False\n",
    "        else:\n",
    "            error = True\n",
    "            word = True\n",
    "        incremental_first = True\n",
    "        #the below does incremental and final output in one, also outputting the final outputs\n",
    "        #derivable from the incremental output, takes quite a while\n",
    "        if incremental_first:\n",
    "            results,speaker_rate_dict,error_analysis = incremental_output_disfluency_eval_from_file(\n",
    "                                                        hyp_dir+\"epoch_{0}/{1}_output_increco.text\"\n",
    "                                                            .format(e,division),\\\n",
    "                                                        gold_data, \n",
    "                                                        utt_eval=False, \n",
    "                                                        error_analysis=error, \n",
    "                                                        word=word, \n",
    "                                                        interval=False,\\\n",
    "                                                        outputfilename=hyp_dir+\\\n",
    "                                                            \"epoch_{0}/predictions_inc_{1}.final\"\n",
    "                                                            .format(e,division))\n",
    "        else:\n",
    "            #the below does just the final output evaluation, assuming a final output file, faster\n",
    "            results,speaker_rate_dict,error_analysis = final_output_disfluency_eval_from_file(\n",
    "                                    hyp_dir+\"epoch_{0}/predictions_inc_{1}.final\".format(e,division),\\\n",
    "                                    gold_data, utt_eval=False, error_analysis=error, word=word, interval=False,\\\n",
    "                                    outputfilename=None)\n",
    "        for k,v in results.items():\n",
    "            print k,v\n",
    "        all_results[division + \"_\" + system] = deepcopy(results)\n",
    "        all_error_dicts[division + \"_\" + system] = deepcopy(error_analysis)\n",
    "    #break #just one system for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heldout_LSTM (complex tags) ************************************\n",
      "p_<rpnsub_word 0.443585780526\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.637700760778\n",
      "r_<rpnsub_word 0.27229601518\n",
      "p_<rpnrep_word 0.891640866873\n",
      "r_<rpndel_word 0.00757575757576\n",
      "f1_<rms_word 0.680493273543\n",
      "delayed_acc_<rm_4_word None\n",
      "p_<rps_relaxed_word 0.979306487696\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.49041400389\n",
      "f1_<rm.<i.<rp_word 0.576229758631\n",
      "delayed_acc_<rm_mean_word None\n",
      "f1_<rps_relaxed_word 0.8748438671\n",
      "p_<rm.<rp.<i_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "p_<rm_word 0.746268656716\n",
      "r_<i_word 0.620202020202\n",
      "f1_<rpndel_word 0.014598540146\n",
      "f1_<rp_word 0.524673008323\n",
      "delayed_acc_<rm_5_word None\n",
      "r_<rps_relaxed_word 0.790519187359\n",
      "pearson_r_correl_rps_rate_per_word nan\n",
      "p_<i_word 0.604330708661\n",
      "r_<e_word 0.886027536971\n",
      "processing_overhead_word None\n",
      "r_<rm_word 0.551410963347\n",
      "p_<rms_word 0.74387254902\n",
      "f1_<rpnrep_word 0.867905575088\n",
      "p_<e_word 0.856122197586\n",
      "p_<rps_word 0.795861297539\n",
      "r_<rpn_word 0.557736193519\n",
      "p_<rp_word 0.564077980185\n",
      "delayed_acc_<rm_1_word None\n",
      "f1_<rps_word 0.710966774919\n",
      "pearson_r_p_value_rps_rate_per_word 1.0\n",
      "f1_<i_word 0.612163509472\n",
      "p_<rpndel_word 0.2\n",
      "r_<rms_word 0.627066115702\n",
      "f1_<rm_word 0.634210035441\n",
      "f1_<e_word 0.870818193209\n",
      "delayed_acc_<rm_2_word None\n",
      "edit_overhead_rel_word 6.30311614731\n",
      "p_<e_relaxed_word 0\n",
      "t_t_detection_<e_word 0.196083231334\n",
      "r_<rps_word 0.642437923251\n",
      "r_<rm.<i.<rp_word 0.525567785983\n",
      "t_t_detection_<rms_word 2.53028064993\n",
      "r_<rpnrep_word 0.845401174168\n",
      "f1_<rpn_word 0.646218931782\n",
      "p_<rpn_word 0.768070395977\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rpnsub_word 0.337448559671\n",
      "r_<e_relaxed_word 0\n",
      "t_t_detection_<rps_word 1.20519159456\n",
      "delayed_acc_<rm_6_word None\n"
     ]
    }
   ],
   "source": [
    "#print out the results\n",
    "for div,results in all_results.items():\n",
    "    print div, \"************************************\"\n",
    "    #all_results[div+ \"(complex tags)\"] = results\n",
    "    for r,v in results.items():\n",
    "        #if v == 0 or v == None: continue\n",
    "        if not \"word\" in r: continue\n",
    "        #if not r[:2] in [\"p_\",\"r_\"]: continue\n",
    "        print r,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heldout_LSTM (complex tags)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>$F_{rm}$ (per word)</th>\n",
       "      <th>$F_{e}$ (per word)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM (complex tags) (transcript)</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              System (eval. method) $F_{rm}$ (per word) $F_{e}$ (per word)\n",
       "0  LSTM (complex tags) (transcript)               0.593              0.901"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter out the heldout files\n",
    "test_only = [x for x in all_results.keys()]\n",
    "print test_only\n",
    "test_results = {k : all_results[k] for k in test_only}\n",
    "#have a look at the test results\n",
    "final = convert_to_latex(test_results, inc=False, eval_level=['word'], utt_seg=False)\n",
    "final = final.drop(final.columns[[-2]], axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>TTD$_{rps}$ (word)</th>\n",
       "      <th>TTD$_{rps}$ (time in s)</th>\n",
       "      <th>TTD$_{tto}$ (word)</th>\n",
       "      <th>TTD$_{tto}$ (time in s)</th>\n",
       "      <th>EO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN (transcript)</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.350</td>\n",
       "      <td>1.814</td>\n",
       "      <td>11.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN (ASR results)</td>\n",
       "      <td>-</td>\n",
       "      <td>0.198</td>\n",
       "      <td>-</td>\n",
       "      <td>3.649</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  System (eval. method) TTD$_{rps}$ (word) TTD$_{rps}$ (time in s)  \\\n",
       "0      RNN (transcript)              1.007                   0.253   \n",
       "1     RNN (ASR results)                  -                   0.198   \n",
       "\n",
       "  TTD$_{tto}$ (word) TTD$_{tto}$ (time in s)     EO  \n",
       "0              0.350                   1.814  11.03  \n",
       "1                  -                   3.649  20.53  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc = convert_to_latex(test_results, inc=True)\n",
    "inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write the final and incremental output to a file\n",
    "latex_file = open(\"latex_output_single_tasks_simple.txt\",\"w\")\n",
    "latex_file.write(inc.to_latex(index=False))\n",
    "latex_file.write(\"\\n\\n\" + final.to_latex(index=False))\n",
    "latex_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_asr_RNN <type 'bool'>\n",
      "test_RNN <type 'dict'>\n",
      "heldout_asr_RNN <type 'bool'>\n",
      "heldout_RNN <type 'dict'>\n",
      "heldout_RNN t>\n",
      "\n",
      "FP 834\n",
      "left context=\n",
      "right context=well|<e/><tt> i|<f/><tc> have|<f/><cc> to|<f/><cc> say|<f/><cc>\n",
      "gold left context=\n",
      "gold right context=well|<e/><tc> i|<f/><cc> have|<f/><cc> to|<f/><cc> say|<f/><cc>\n",
      "type = None\n",
      "\n",
      "TP 3264\n",
      "left context=\n",
      "right context=turn|<f/><ct> well|<e/><tt> i|<f/><tc> have|<f/><cc> to|<f/><cc>\n",
      "gold left context=\n",
      "gold right context=turn|<f/><ct> well|<e/><tc> i|<f/><cc> have|<f/><cc> to|<f/><cc>\n",
      "type = None\n",
      "\n",
      "FN 2468\n",
      "left context=i|<f/><cc> really|<f/><cc> dont|<f/><cc> have|<f/><cc> a|<f/><cc>\n",
      "right context=budget|<f/><cc> both|<f/><cc> my|<f/><cc> wife|<f/><cc> and|<f/><cc>\n",
      "gold left context=i|<f/><cc> really|<f/><cc> dont|<f/><cc> have|<f/><cc> a|<f/><cc>\n",
      "gold right context=budget|<f/><ct> both|<f/><tc> my|<f/><cc> wife|<f/><cc> and|<f/><cc>\n",
      "type = None\n",
      "heldout_RNN <rps\n",
      "heldout_RNN <e\n",
      "defaultdict(<type 'int'>, {})\n",
      "defaultdict(<type 'int'>, {})\n",
      "CC & 932 & 16.26\n",
      "subj & 400 & 6.98\n",
      "<e & 279 & 4.87\n",
      "<rps & 165 & 2.88\n",
      " & 71 & 1.24\n",
      "proper_other & 66 & 1.15\n",
      "it & 64 & 1.12\n",
      "ack & 60 & 1.05\n",
      "thats & 20 & 0.35\n",
      "if & 18 & 0.31\n",
      "what & 17 & 0.30\n",
      "my & 17 & 0.30\n",
      "theres & 13 & 0.23\n",
      "when & 12 & 0.21\n",
      "a & 11 & 0.19\n",
      "how & 10 & 0.17\n",
      "do & 10 & 0.17\n",
      "there & 9 & 0.16\n",
      "this & 9 & 0.16\n",
      "is & 9 & 0.16\n",
      "maybe & 9 & 0.16\n",
      "in & 8 & 0.14\n",
      "at & 8 & 0.14\n",
      "theyre & 7 & 0.12\n",
      "th- & 7 & 0.12\n",
      "to & 6 & 0.10\n",
      "then & 6 & 0.10\n",
      "of & 6 & 0.10\n",
      "people & 5 & 0.09\n",
      "probably & 5 & 0.09\n",
      "lets & 4 & 0.07\n",
      "every & 4 & 0.07\n",
      "not & 4 & 0.07\n",
      "like & 4 & 0.07\n",
      "for & 4 & 0.07\n",
      "hes & 4 & 0.07\n",
      "have & 4 & 0.07\n",
      "as & 4 & 0.07\n",
      "go & 3 & 0.05\n",
      "just & 3 & 0.05\n",
      "actually & 3 & 0.05\n",
      "pretty & 3 & 0.05\n",
      "now & 3 & 0.05\n",
      "where & 3 & 0.05\n",
      "an- & 3 & 0.05\n",
      "on & 3 & 0.05\n",
      "whats & 3 & 0.05\n",
      "were & 3 & 0.05\n",
      "youre & 3 & 0.05\n",
      "a- & 3 & 0.05\n",
      "all & 2 & 0.03\n",
      "particularly & 2 & 0.03\n",
      "both & 2 & 0.03\n",
      "somebody & 2 & 0.03\n",
      "possibly & 2 & 0.03\n",
      "weve & 2 & 0.03\n",
      "are & 2 & 0.03\n",
      "our & 2 & 0.03\n",
      "really & 2 & 0.03\n",
      "even & 2 & 0.03\n",
      "who & 2 & 0.03\n",
      "about & 2 & 0.03\n",
      "was & 2 & 0.03\n",
      "sort & 2 & 0.03\n",
      "those & 2 & 0.03\n",
      "theyve & 2 & 0.03\n",
      "up & 2 & 0.03\n",
      "didnt & 2 & 0.03\n",
      "things & 2 & 0.03\n",
      "sure & 2 & 0.03\n",
      "o- & 2 & 0.03\n",
      "most & 2 & 0.03\n",
      "i- & 2 & 0.03\n",
      "why & 2 & 0.03\n",
      "yo- & 2 & 0.03\n",
      "pardon & 1 & 0.02\n",
      "unemployment & 1 & 0.02\n",
      "being & 1 & 0.02\n",
      "wed & 1 & 0.02\n",
      "had & 1 & 0.02\n",
      "late & 1 & 0.02\n",
      "good & 1 & 0.02\n",
      "big & 1 & 0.02\n",
      "television & 1 & 0.02\n",
      "ones & 1 & 0.02\n",
      "course & 1 & 0.02\n",
      "schools & 1 & 0.02\n",
      "taxes & 1 & 0.02\n",
      "did & 1 & 0.02\n",
      "bad & 1 & 0.02\n",
      "either & 1 & 0.02\n",
      "went & 1 & 0.02\n",
      "someday & 1 & 0.02\n",
      "w- & 1 & 0.02\n",
      "theyd & 1 & 0.02\n",
      "some & 1 & 0.02\n",
      "back & 1 & 0.02\n",
      "belie- & 1 & 0.02\n",
      "heres & 1 & 0.02\n",
      "ac- & 1 & 0.02\n",
      "looking & 1 & 0.02\n",
      "goes & 1 & 0.02\n",
      "be & 1 & 0.02\n",
      "never & 1 & 0.02\n",
      "here & 1 & 0.02\n",
      "let & 1 & 0.02\n",
      "plea & 1 & 0.02\n",
      "put & 1 & 0.02\n",
      "by & 1 & 0.02\n",
      "service & 1 & 0.02\n",
      "teachers & 1 & 0.02\n",
      "ill & 1 & 0.02\n",
      "pe- & 1 & 0.02\n",
      "arent & 1 & 0.02\n",
      "youve & 1 & 0.02\n",
      "p- & 1 & 0.02\n",
      "doesnt & 1 & 0.02\n",
      "another & 1 & 0.02\n",
      "bu- & 1 & 0.02\n",
      "drove & 1 & 0.02\n",
      "tha- & 1 & 0.02\n",
      "everybody & 1 & 0.02\n",
      "from & 1 & 0.02\n",
      "would & 1 & 0.02\n",
      "hey & 1 & 0.02\n",
      "their & 1 & 0.02\n",
      "coarse & 1 & 0.02\n",
      "tell & 1 & 0.02\n",
      "selling & 1 & 0.02\n",
      "boy & 1 & 0.02\n",
      "fire & 1 & 0.02\n",
      "nobody & 1 & 0.02\n",
      "huh & 1 & 0.02\n",
      "ended & 1 & 0.02\n",
      "with & 1 & 0.02\n",
      "woman & 1 & 0.02\n",
      "kind & 1 & 0.02\n",
      "unfortunately & 1 & 0.02\n",
      "shes & 1 & 0.02\n",
      "while & 1 & 0.02\n",
      "supposed & 1 & 0.02\n",
      "called & 1 & 0.02\n",
      "tonight & 1 & 0.02\n",
      "say & 1 & 0.02\n",
      "his & 1 & 0.02\n",
      "something & 1 & 0.02\n",
      "apparently & 1 & 0.02\n",
      "any & 1 & 0.02\n",
      "again & 1 & 0.02\n",
      "theyll & 1 & 0.02\n",
      "same & 1 & 0.02\n",
      "other & 1 & 0.02\n",
      "which & 1 & 0.02\n",
      "higher & 1 & 0.02\n",
      "gas & 1 & 0.02\n",
      "after & 1 & 0.02\n",
      "nothing & 1 & 0.02\n",
      "give & 1 & 0.02\n",
      "especially & 1 & 0.02\n",
      "sometimes & 1 & 0.02\n",
      "well & 1 & 0.02\n",
      "definitely & 1 & 0.02\n",
      "putting & 1 & 0.02\n",
      "usu- & 1 & 0.02\n",
      "first & 1 & 0.02\n",
      "once & 1 & 0.02\n",
      "total & 2468 & 43.06\n"
     ]
    }
   ],
   "source": [
    "#Error analyses\n",
    "error = {\"TP\" : {}, \"FP\" : {}, \"FN\": {} }\n",
    " \n",
    "for div,all_error in all_error_dicts.items():\n",
    "    print div, type(all_error)\n",
    "    if type(all_error) == bool: continue\n",
    "    if \"test\" in div: continue\n",
    "    #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "    for tag,errors in all_error.items():\n",
    "        print div, tag\n",
    "        #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "        if not tag == \"t>\": continue\n",
    "        for k,v in errors.items():\n",
    "    \n",
    "            print \"\"\n",
    "            #if k == \"FP\": continue\n",
    "            print k, len(v)\n",
    "            typedict = defaultdict(int)\n",
    "            lendict = defaultdict(int)\n",
    "            print v[0]\n",
    "            for repair in v:\n",
    "                #if len(repair)==0: continue\n",
    "                #print \"*\"\n",
    "                #print repair\n",
    "                #print repair.gold_context\n",
    "                onset = \"\"\n",
    "                if tag == \"<rps\":\n",
    "                    \n",
    "                    for i in rcange(0,len(repair.gold_context)):\n",
    "                        if repair.gold_context[i] == \"+|+\":\n",
    "                            onset = repair.gold_context[i+1]\n",
    "                            break\n",
    "                else:\n",
    "                    gold_onset = \"\"\n",
    "                    onset = \"\"\n",
    "                    word = \"\"\n",
    "                    if len(repair.gold_tags_right_context)>1:\n",
    "                        gold_onset = repair.gold_tags_right_context[1]\n",
    "                        onset = repair.tags_right_context[1]\n",
    "                        word = repair.words_right_context[1]\n",
    "                    #penult = repair.tags_left_context[-1]\n",
    "                    #print repair\n",
    "                    if k == \"FP\":\n",
    "                        onset = gold_onset\n",
    "                    if \"<rps\" in onset:\n",
    "                        typedict[\"<rps\"]+=1\n",
    "                    elif \"<e\" in onset:\n",
    "                        typedict[\"<e\"]+=1\n",
    "                    else:\n",
    "                        if word in [\"and\",\"or\",\"but\",\"so\",\"because\",\"that\",\"although\"]:\n",
    "                            typedict[\"CC\"]+=1\n",
    "                        elif word in [\"i\",\"we\",\"they\",\"im\",\"ive\",\"he\",\"she\",\"id\"]:\n",
    "                            typedict[\"subj\"]+=1\n",
    "                        elif word in [\"you\",\"the\"] or \"$\" in word:\n",
    "                            typedict[\"proper_other\"]+=1\n",
    "                        elif word in [\"yeah\",\"no\",\"okay\",\"yes\",\"right\",\"uh-huh\"]:\n",
    "                            typedict[\"ack\"]+=1\n",
    "                        elif word in [\"it\",\"its\"]:\n",
    "                            typedict[\"it\"]+=1\n",
    "                        else:\n",
    "                            typedict[word]+=1\n",
    "                         \n",
    "                \n",
    "                #if \"<t\" in repair.gold_context\n",
    "                #for t in [\"tt\",\"cc\",\"ct\",\"tc\"]:\n",
    "                #    if \"<\" + t + \">\" in onset:\n",
    "                #        typedict[t]+=1\n",
    "                #        if not t[0]=='t':\n",
    "                #            print repair\n",
    "                if tag == \"<rps\" and not k == 'FP':\n",
    "                    lendict[repair.type]+=1\n",
    "                error[k]['len'] = deepcopy(lendict)\n",
    "                error[k]['type'] = deepcopy(typedict)\n",
    "\n",
    "#tp = deepcopy(lendict)\n",
    "#q1. THE RECALL RATES FOR VARIOUS GOLD REPAIRS\n",
    "tp = error['TP']['len']\n",
    "print tp\n",
    "print error['FN']['len']\n",
    "for k,v in sorted(error['FN']['len'].items()):\n",
    "    print \" & \".join([k, \"({0})\".format(v + tp[k]), \n",
    "                      '{0:.1f}'.format(100 * float(tp[k])/float(v+ tp[k]))]) + \"\\\\\\\\\"\n",
    "\n",
    "tps = error['TP']['type']\n",
    "fns = error['FN']['type']\n",
    "fps = error['FP']['type']\n",
    "\n",
    "total = sum(fns.values()+tps.values())\n",
    "\n",
    "errormass = 0\n",
    "errortotal = 0\n",
    "for k,v in sorted(fns.items(),key= lambda x: x[1],reverse=True):\n",
    "    print k,\"&\",v,\"&\",'{0:.2f}'.format(v/total * 100)\n",
    "    errormass +=(v/total * 100)\n",
    "    errortotal+=v\n",
    "print \"total &\",errortotal,\"&\",'{0:.2f}'.format(errormass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #TODO for paper/future\n",
    "# - check WER for ASR results and exclude those with high ones given they might have high overlap :(\n",
    "# - need to adjust the time to detection scores based on the time it comes in from Increco?? \n",
    "#      Also for ttdetection can only use word ends unless we re-do the mapping- just needs explanation\n",
    "# - delayed accuracy based on time, or not bother? do moving window instead and plot this over time- average moving window accuracy\n",
    "# - error analysis plots\n",
    "# 036- full task with LSTM- should improve massively over 034, which also needs re-running\n",
    "# Reproduce 027 (with full training data, efficiently) and re-run with LSTM- not much time.\n",
    "#Q2 TODO the extent to which the network is memorizing- need to plug these in with the repair gold standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
