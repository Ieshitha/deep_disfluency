{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computes the accuracies for the outputs from the EACL 2017 experiments on\n",
    "#joint incremental utterance segmentation and disfluency detection\n",
    "#this assumes the experiments are in simple_rnn_disf/rnn_disf_detection/experiments/\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "sys.path.append(\"../../../../\")\n",
    "# from mumodo.mumodoIO import open_intervalframe_from_textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add the evaluation module functions\n",
    "from deep_disfluency.evaluation.disf_evaluation import incremental_output_disfluency_eval_from_file\n",
    "from deep_disfluency.evaluation.disf_evaluation import final_output_disfluency_eval_from_file\n",
    "from deep_disfluency.evaluation.eval_utils import get_tag_data_from_corpus_file\n",
    "from deep_disfluency.evaluation.eval_utils import rename_all_repairs_in_line_with_index\n",
    "from deep_disfluency.evaluation.eval_utils import sort_into_dialogue_speakers\n",
    "from deep_disfluency.evaluation.results_utils import convert_to_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the locations of all needed files\n",
    "# Assume we have the incremental output\n",
    "experiment_dir = \"../../../experiments\"\n",
    "\n",
    "partial_words = True  # No partial words in these experiments, removed\n",
    "if partial_words:\n",
    "    partial = '_partial'\n",
    "else:\n",
    "    partial = ''\n",
    "#the evaluation files (as text files)\n",
    "disf_dir = \"../../../data/disfluency_detection/switchboard\"\n",
    "disfluency_files = [\n",
    "                    disf_dir + \"/swbd_disf_heldout{}_data_timings.csv\".format(partial),\n",
    "                    disf_dir + \"/swbd_disf_test{}_data_timings.csv\".format(partial)\n",
    "                   ]\n",
    "allsystemsfinal = [\n",
    "                 (\"033/epoch_45\",'RNN (timing)'),\n",
    "                 (\"035/epoch_6\",'LSTM (timing)'),\n",
    "                 #  (\"036/epoch_15\",'LSTM (complex tags)'),\n",
    "                 #  (\"037/epoch_6\",'LSTM (disf only) (timing)'),\n",
    "                 #  (\"038/epoch_8\",'LSTM (TTO only) (timing)'),\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_asr_heldout = [line.strip(\"\\n\") for line in open(\n",
    "        \"../../../data/disfluency_detection/swda_divisions_disfluency_detection/swbd_disf_heldout_ASR_good_ranges.text\")]\n",
    "good_asr_test = [line.strip(\"\\n\") for line in open(\n",
    "        \"../../../data/disfluency_detection/swda_divisions_disfluency_detection/swbd_disf_test_ASR_good_ranges.text\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "033/epoch_45\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_partial_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= True utt_eval= True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/.local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing final output to file ../../../experiments/033/epoch_45/swbd_disf_heldout_partial_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "t_t_detection_<e_interval 0.271064261422\n",
      "t_t_detection_<rps_interval 0.189058147714\n",
      "t_t_detection_<rms_interval nan\n",
      "edit_overhead_rel_interval 2.93701049825\n",
      "delayed_acc_<rm_4_interval None\n",
      "t_t_detection_final_t/>_interval None\n",
      "delayed_acc_<rm_4_word None\n",
      "delayed_acc_<rm_1_word None\n",
      "t_t_detection_final_t/>_word None\n",
      "t_t_detection_t/>_interval 1.01139070944\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word 0.269863013699\n",
      "delayed_acc_<rm_5_interval None\n",
      "delayed_acc_<rm_2_interval None\n",
      "delayed_acc_<rm_1_interval None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "delayed_acc_<rm_mean_interval None\n",
      "processing_overhead_interval None\n",
      "t_t_detection_<e_word 0.0169444444444\n",
      "delayed_acc_<rm_6_interval None\n",
      "t_t_detection_<rms_word nan\n",
      "edit_overhead_rel_word 2.93701049825\n",
      "delayed_acc_<rm_5_word None\n",
      "delayed_acc_<rm_3_interval None\n",
      "t_t_detection_<rps_word 1.00075471698\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_partial_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= True utt_eval= True\n",
      "writing final output to file ../../../experiments/033/epoch_45/swbd_disf_test_partial_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "t_t_detection_<e_interval 0.287048358712\n",
      "t_t_detection_<rps_interval 0.197643040343\n",
      "t_t_detection_<rms_interval nan\n",
      "edit_overhead_rel_interval 3.33967222923\n",
      "delayed_acc_<rm_4_interval None\n",
      "t_t_detection_final_t/>_interval None\n",
      "delayed_acc_<rm_4_word None\n",
      "delayed_acc_<rm_1_word None\n",
      "t_t_detection_final_t/>_word None\n",
      "t_t_detection_t/>_interval 1.0601321534\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word 0.269503546099\n",
      "delayed_acc_<rm_5_interval None\n",
      "delayed_acc_<rm_2_interval None\n",
      "delayed_acc_<rm_1_interval None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "delayed_acc_<rm_mean_interval None\n",
      "processing_overhead_interval None\n",
      "t_t_detection_<e_word 0.0260005842828\n",
      "delayed_acc_<rm_6_interval None\n",
      "t_t_detection_<rms_word nan\n",
      "edit_overhead_rel_word 3.33967222923\n",
      "delayed_acc_<rm_5_word None\n",
      "delayed_acc_<rm_3_interval None\n",
      "t_t_detection_<rps_word 1.00417101147\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n",
      "035/epoch_6\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_partial_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= True utt_eval= True\n",
      "writing final output to file ../../../experiments/035/epoch_6/swbd_disf_heldout_partial_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "t_t_detection_<e_interval 0.252450470517\n",
      "t_t_detection_<rps_interval 0.18963096313\n",
      "t_t_detection_<rms_interval nan\n",
      "edit_overhead_rel_interval 3.1953007832\n",
      "delayed_acc_<rm_4_interval None\n",
      "t_t_detection_final_t/>_interval None\n",
      "delayed_acc_<rm_4_word None\n",
      "delayed_acc_<rm_1_word None\n",
      "t_t_detection_final_t/>_word None\n",
      "t_t_detection_t/>_interval 1.2549532724\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word 0.327500825355\n",
      "delayed_acc_<rm_5_interval None\n",
      "delayed_acc_<rm_2_interval None\n",
      "delayed_acc_<rm_1_interval None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "delayed_acc_<rm_mean_interval None\n",
      "processing_overhead_interval None\n",
      "t_t_detection_<e_word 0.0123796423659\n",
      "delayed_acc_<rm_6_interval None\n",
      "t_t_detection_<rms_word nan\n",
      "edit_overhead_rel_word 3.1953007832\n",
      "delayed_acc_<rm_5_word None\n",
      "delayed_acc_<rm_3_interval None\n",
      "t_t_detection_<rps_word 1.00071479628\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_partial_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "incremental output disfluency evaluation\n",
      "word= True interval= True utt_eval= True\n",
      "writing final output to file ../../../experiments/035/epoch_6/swbd_disf_test_partial_data_output_final.text\n",
      "edit_overhead_rel_<rm None\n",
      "t_t_detection_<e_interval 0.265220228657\n",
      "t_t_detection_<rps_interval 0.201595023254\n",
      "t_t_detection_<rms_interval nan\n",
      "edit_overhead_rel_interval 3.55120617081\n",
      "delayed_acc_<rm_4_interval None\n",
      "t_t_detection_final_t/>_interval None\n",
      "delayed_acc_<rm_4_word None\n",
      "delayed_acc_<rm_1_word None\n",
      "t_t_detection_final_t/>_word None\n",
      "t_t_detection_t/>_interval 1.18537956755\n",
      "edit_overhead_rel_tto None\n",
      "t_t_detection_t/>_word 0.328869047619\n",
      "delayed_acc_<rm_5_interval None\n",
      "delayed_acc_<rm_2_interval None\n",
      "delayed_acc_<rm_1_interval None\n",
      "delayed_acc_<rm_mean_word None\n",
      "delayed_acc_<rm_3_word None\n",
      "delayed_acc_<rm_2_word None\n",
      "delayed_acc_<rm_mean_interval None\n",
      "processing_overhead_interval None\n",
      "t_t_detection_<e_word 0.0170421721548\n",
      "delayed_acc_<rm_6_interval None\n",
      "t_t_detection_<rms_word nan\n",
      "edit_overhead_rel_word 3.55120617081\n",
      "delayed_acc_<rm_5_word None\n",
      "delayed_acc_<rm_3_interval None\n",
      "t_t_detection_<rps_word 1.00197238659\n",
      "delayed_acc_<rm_6_word None\n",
      "processing_overhead_word None\n"
     ]
    }
   ],
   "source": [
    "# create final output files for the final output evaluation (and do incremental evaluation first:\n",
    "# NB this takes a while!\n",
    "DO_INCREMENTAL_EVAL = True\n",
    "if DO_INCREMENTAL_EVAL:\n",
    "    all_incremental_results = {}\n",
    "    all_incremental_error_dicts = {}\n",
    "    for system, system_name in allsystemsfinal:\n",
    "        print system\n",
    "        #if 'complex' in system: break\n",
    "        hyp_dir = experiment_dir + \"/\" + system\n",
    "        #hyp_dir = experiment_dir\n",
    "        for division, disf_file in zip([\"heldout\",\"test\"], disfluency_files):\n",
    "            print \"*\" * 30, division, \"*\" * 30\n",
    "            IDs, timings, words, pos_tags, labels = get_tag_data_from_corpus_file(disf_file)\n",
    "            gold_data = {} #map from the file name to the data\n",
    "            for dialogue,a,b,c,d in zip(IDs, timings, words, pos_tags, labels):\n",
    "                # if \"asr\" in division and not dialogue[:4] in good_asr: continue\n",
    "                gold_data[dialogue] = (a,b,c,d)\n",
    "            inc_filename = hyp_dir + \"/swbd_disf_{0}{1}_data_output_increco\".format(division, partial) + \".text\"\n",
    "            final_output_name = inc_filename.replace(\"_increco\", \"_final\")\n",
    "            results, error_analysis = incremental_output_disfluency_eval_from_file(\n",
    "                                                                             inc_filename,\n",
    "                                                                             gold_data,\n",
    "                                                                             utt_eval=True,\n",
    "                                                                             error_analysis=True,\n",
    "                                                                             word=True,\n",
    "                                                                             interval=True,\n",
    "                                                                             outputfilename=final_output_name)\n",
    "            for k,v in results.items():\n",
    "                print k,v\n",
    "            all_incremental_results[division + \"_\" + system] = deepcopy(results)\n",
    "            if \"heldout\" in division:\n",
    "                # only do the error analyses on the heldout data\n",
    "                all_incremental_error_dicts[division + \"_\" + system] = deepcopy(error_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>TTD$_{tto}$ (time in s)</th>\n",
       "      <th>TTD$_{rps}$ (word)</th>\n",
       "      <th>EO (word)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM joint task (timing) (transcript)</td>\n",
       "      <td>1.185</td>\n",
       "      <td>1.002</td>\n",
       "      <td>3.551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   System (eval. method) TTD$_{tto}$ (time in s)  \\\n",
       "0  LSTM joint task (timing) (transcript)                   1.185   \n",
       "\n",
       "  TTD$_{rps}$ (word) EO (word)  \n",
       "0              1.002     3.551  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = \"No incremental results here\"\n",
    "if DO_INCREMENTAL_EVAL:\n",
    "    display_results = dict()\n",
    "    # display_results['RNN joint task (timing)'] = all_incremental_results['test_033/epoch_45']\n",
    "    display_results['LSTM joint task (timing)'] = all_incremental_results['test_035/epoch_6']\n",
    "    final = convert_to_latex(display_results, eval_level=['word'], inc=True, utt_seg=False, only_include=\n",
    "                            ['t_t_detection_t/>_interval', 't_t_detection_<rps_word', 'edit_overhead_rel_word'])\n",
    "    #final = final.drop(final.columns[[-2]], axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final output evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "033/epoch_45\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_partial_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n",
      "word\n",
      "pearson_r_p_value_rps_number 2.00127665348e-57\n",
      "p_<rm_word 0\n",
      "spearman_rank_p_value_rps_rate_per_word 7.61361432954e-39\n",
      "r_<rpnsub_word 0\n",
      "pearson_r_p_value_rps_rate_per_utt 2.12135073355e-47\n",
      "p_t/>_word 0.766506922258\n",
      "f1_t/>_word 0.612292641429\n",
      "r_<rm.<i.<rp_word 0.184617528215\n",
      "DSER_word 69.8123229462\n",
      "r_<i_word 0\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.510400616333\n",
      "pearson_r_correl_rps_number 0.960620179499\n",
      "p_<rpnrep_word 0\n",
      "p_<i_word 0\n",
      "r_<e_word 0.901325854156\n",
      "p_<rpnsub_word 0\n",
      "r_t/>_relaxed_word 0\n",
      "SegER None\n",
      "r_<rm_word 0\n",
      "p_<rms_word 0\n",
      "p_<rps_word 0.827997489014\n",
      "r_<rpn_word 0\n",
      "pearson_r_p_value_rps_rate_per_word 1.03789571867e-47\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0\n",
      "f1_<e_word 0.913200723327\n",
      "NIST_SU None\n",
      "r_<rps_word 0.595485327314\n",
      "f1_<rpn_word 0\n",
      "f1_t/>_relaxed_word 0\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.831763967357\n",
      "spearman_rank_correl_rps_rate_per_utt 0.875289821846\n",
      "\\ r_t/>_relaxed_word None\n",
      "NIST_SU_word 64.5538243626\n",
      "p_<rps_relaxed_word 0.902699309479\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.368157821617\n",
      "p_t/>_relaxed_word 0\n",
      "r_<rpndel_word 0\n",
      "f1_<rm.<i.<rp_word 0.302166476625\n",
      "f1_<rps_relaxed_word 0.75525210084\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0\n",
      "r_<rps_relaxed_word 0.64920993228\n",
      "spearman_rank_p_value_rps_number 2.88748139032e-42\n",
      "p_<rpn_word 0\n",
      "f1_<rms_word 0\n",
      "f1_<rpnrep_word 0\n",
      "f1_<rpnsub_word 0\n",
      "r_<rpnrep_word 0\n",
      "f1_<rps_word 0.69275210084\n",
      "f1_<i_word 0\n",
      "spearman_rank_correl_rps_rate_per_word 0.904735921022\n",
      "pearson_r_correl_rps_rate_per_utt 0.936776246916\n",
      "DSER None\n",
      "r_t/>_word 0.50973796034\n",
      "pearson_r_correl_rps_rate_per_word 0.937702318949\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.925392670157\n",
      "p_<rm.<rp.<i_word None\n",
      "p_<rp_word 0.831763967357\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.919210572958\n",
      "spearman_rank_p_value_rps_rate_per_utt 2.54640170682e-33\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_partial_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n",
      "word\n",
      "pearson_r_p_value_rps_number 2.46816937793e-45\n",
      "p_<rm_word 0\n",
      "spearman_rank_p_value_rps_rate_per_word 4.53986817865e-24\n",
      "r_<rpnsub_word 0\n",
      "pearson_r_p_value_rps_rate_per_utt 9.47151807532e-27\n",
      "p_t/>_word 0.77640503876\n",
      "f1_t/>_word 0.641256502601\n",
      "r_<rm.<i.<rp_word 0.161735036987\n",
      "DSER_word 67.0245398773\n",
      "r_<i_word 0\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.464734299517\n",
      "pearson_r_correl_rps_number 0.933147504039\n",
      "p_<rpnrep_word 0\n",
      "p_<i_word 0\n",
      "r_<e_word 0.894496644295\n",
      "p_<rpnsub_word 0\n",
      "r_t/>_relaxed_word 0\n",
      "SegER None\n",
      "r_<rm_word 0\n",
      "p_<rms_word 0\n",
      "p_<rps_word 0.788254755997\n",
      "r_<rpn_word 0\n",
      "pearson_r_p_value_rps_rate_per_word 3.21347192636e-29\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0\n",
      "f1_<e_word 0.904574453645\n",
      "NIST_SU None\n",
      "r_<rps_word 0.539943342776\n",
      "f1_<rpn_word 0\n",
      "f1_t/>_relaxed_word 0\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.795698924731\n",
      "spearman_rank_correl_rps_rate_per_utt 0.850497352819\n",
      "\\ r_t/>_relaxed_word None\n",
      "NIST_SU_word 61.1111111111\n",
      "p_<rps_relaxed_word 0.863523573201\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.328215626066\n",
      "p_t/>_relaxed_word 0\n",
      "r_<rpndel_word 0\n",
      "f1_<rm.<i.<rp_word 0.268827721112\n",
      "f1_<rps_relaxed_word 0.702084734364\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0\n",
      "r_<rps_relaxed_word 0.591501416431\n",
      "spearman_rank_p_value_rps_number 7.78701798745e-34\n",
      "p_<rpn_word 0\n",
      "f1_<rms_word 0\n",
      "f1_<rpnrep_word 0\n",
      "f1_<rpnsub_word 0\n",
      "r_<rpnrep_word 0\n",
      "f1_<rps_word 0.640887693342\n",
      "f1_<i_word 0\n",
      "spearman_rank_correl_rps_rate_per_word 0.806213039947\n",
      "pearson_r_correl_rps_rate_per_utt 0.831395382199\n",
      "DSER None\n",
      "r_t/>_word 0.546182685753\n",
      "pearson_r_correl_rps_rate_per_word 0.851422024624\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.914881933004\n",
      "p_<rm.<rp.<i_word None\n",
      "p_<rp_word 0.795698924731\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.882269427528\n",
      "spearman_rank_p_value_rps_rate_per_utt 4.25432419668e-29\n",
      "035/epoch_6\n",
      "****************************** heldout ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_heldout_partial_data_timings.csv\n",
      "loaded 102 sequences\n",
      "102 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n",
      "word\n",
      "pearson_r_p_value_rps_number 3.10661564561e-55\n",
      "p_<rm_word 0\n",
      "spearman_rank_p_value_rps_rate_per_word 2.29912382915e-35\n",
      "r_<rpnsub_word 0\n",
      "pearson_r_p_value_rps_rate_per_utt 6.14052349529e-47\n",
      "p_t/>_word 0.860069244085\n",
      "f1_t/>_word 0.654158437569\n",
      "r_<rm.<i.<rp_word 0.195903580883\n",
      "DSER_word 67.492917847\n",
      "r_<i_word 0\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.525705739391\n",
      "pearson_r_correl_rps_number 0.956348137739\n",
      "p_<rpnrep_word 0\n",
      "p_<i_word 0\n",
      "r_<e_word 0.914584395716\n",
      "p_<rpnsub_word 0\n",
      "r_t/>_relaxed_word 0\n",
      "SegER None\n",
      "r_<rm_word 0\n",
      "p_<rms_word 0\n",
      "p_<rps_word 0.798857142857\n",
      "r_<rpn_word 0\n",
      "pearson_r_p_value_rps_rate_per_word 1.62353263662e-45\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0\n",
      "f1_<e_word 0.922582304527\n",
      "NIST_SU None\n",
      "r_<rps_word 0.631151241535\n",
      "f1_<rpn_word 0\n",
      "f1_t/>_relaxed_word 0\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.803428571429\n",
      "spearman_rank_correl_rps_rate_per_utt 0.875095779532\n",
      "\\ r_t/>_relaxed_word None\n",
      "NIST_SU_word 55.8073654391\n",
      "p_<rps_relaxed_word 0.874285714286\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.390664073354\n",
      "p_t/>_relaxed_word 0\n",
      "r_<rpndel_word 0\n",
      "f1_<rm.<i.<rp_word 0.314999439901\n",
      "f1_<rps_relaxed_word 0.771752837327\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0\n",
      "r_<rps_relaxed_word 0.690744920993\n",
      "spearman_rank_p_value_rps_number 4.07247406557e-38\n",
      "p_<rpn_word 0\n",
      "f1_<rms_word 0\n",
      "f1_<rpnrep_word 0\n",
      "f1_<rpnsub_word 0\n",
      "r_<rpnrep_word 0\n",
      "f1_<rps_word 0.705170239596\n",
      "f1_<i_word 0\n",
      "spearman_rank_correl_rps_rate_per_word 0.88718058909\n",
      "pearson_r_correl_rps_rate_per_utt 0.935372972997\n",
      "DSER None\n",
      "r_t/>_word 0.527797450425\n",
      "pearson_r_correl_rps_rate_per_word 0.930843061004\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.93072132849\n",
      "p_<rm.<rp.<i_word None\n",
      "p_<rp_word 0.803428571429\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.901316924421\n",
      "spearman_rank_p_value_rps_rate_per_utt 2.73863685984e-33\n",
      "****************************** test ******************************\n",
      "loading data ../../../data/disfluency_detection/switchboard/swbd_disf_test_partial_data_timings.csv\n",
      "loaded 100 sequences\n",
      "100 speakers\n",
      "final output disfluency evaluation\n",
      "word= True interval= False utt_eval= True\n",
      "word\n",
      "pearson_r_p_value_rps_number 1.72275963486e-42\n",
      "p_<rm_word 0\n",
      "spearman_rank_p_value_rps_rate_per_word 4.62722516958e-24\n",
      "r_<rpnsub_word 0\n",
      "pearson_r_p_value_rps_rate_per_utt 4.63551675844e-38\n",
      "p_t/>_word 0.866666666667\n",
      "f1_t/>_word 0.683998761993\n",
      "r_<rm.<i.<rp_word 0.171149966375\n",
      "DSER_word 64.263803681\n",
      "r_<i_word 0\n",
      "f1_<rpndel_word 0\n",
      "f1_<rp_word 0.484070375654\n",
      "pearson_r_correl_rps_number 0.923210682998\n",
      "p_<rpnrep_word 0\n",
      "p_<i_word 0\n",
      "r_<e_word 0.91355704698\n",
      "p_<rpnsub_word 0\n",
      "r_t/>_relaxed_word 0\n",
      "SegER None\n",
      "r_<rm_word 0\n",
      "p_<rms_word 0\n",
      "p_<rps_word 0.793725490196\n",
      "r_<rpn_word 0\n",
      "pearson_r_p_value_rps_rate_per_word 1.18399603467e-29\n",
      "p_<rpndel_word 0\n",
      "r_<rms_word 0\n",
      "f1_<e_word 0.915277030662\n",
      "NIST_SU None\n",
      "r_<rps_word 0.573371104816\n",
      "f1_<rpn_word 0\n",
      "f1_t/>_relaxed_word 0\n",
      "f1_<e_relaxed_word 0\n",
      "p_<rm.<i.<rp_word 0.798431372549\n",
      "spearman_rank_correl_rps_rate_per_utt 0.87314480903\n",
      "\\ r_t/>_relaxed_word None\n",
      "NIST_SU_word 52.1983640082\n",
      "p_<rps_relaxed_word 0.862745098039\n",
      "r_<rm.<rp.<i_word None\n",
      "r_<rp_word 0.347321733197\n",
      "p_t/>_relaxed_word 0\n",
      "r_<rpndel_word 0\n",
      "f1_<rm.<i.<rp_word 0.281877336287\n",
      "f1_<rps_relaxed_word 0.723684210526\n",
      "f1_<rm.<rp.<i_word None\n",
      "f1_<rm_word 0\n",
      "r_<rps_relaxed_word 0.623229461756\n",
      "spearman_rank_p_value_rps_number 6.20531278654e-34\n",
      "p_<rpn_word 0\n",
      "f1_<rms_word 0\n",
      "f1_<rpnrep_word 0\n",
      "f1_<rpnsub_word 0\n",
      "r_<rpnrep_word 0\n",
      "f1_<rps_word 0.665789473684\n",
      "f1_<i_word 0\n",
      "spearman_rank_correl_rps_rate_per_word 0.806129031294\n",
      "pearson_r_correl_rps_rate_per_utt 0.904552713544\n",
      "DSER None\n",
      "r_t/>_word 0.564928425358\n",
      "pearson_r_correl_rps_rate_per_word 0.854662293572\n",
      "p_<e_relaxed_word 0\n",
      "p_<e_word 0.917003503099\n",
      "p_<rm.<rp.<i_word None\n",
      "p_<rp_word 0.798431372549\n",
      "r_<e_relaxed_word 0\n",
      "spearman_rank_correl_rps_number 0.882848189091\n",
      "spearman_rank_p_value_rps_rate_per_utt 2.4037519535e-32\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = True\n",
    "all_results = {}\n",
    "all_error_dicts = {}\n",
    "for system, system_name in allsystemsfinal:\n",
    "    print system\n",
    "    #if 'complex' in system: break\n",
    "    hyp_dir = experiment_dir\n",
    "    for division, disf_file in zip([\"heldout\", \"test\"],disfluency_files):\n",
    "        #if not division == \"heldout\": continue\n",
    "        print \"*\" * 30, division, \"*\" * 30\n",
    "        IDs, timings, words, pos_tags, labels = get_tag_data_from_corpus_file(disf_file)\n",
    "        gold_data = {} #map from the file name to the data\n",
    "        for dialogue,a,b,c,d in zip(IDs, timings, words, pos_tags, labels):\n",
    "            # if \"asr\" in division and not dialogue[:4] in good_asr: continue\n",
    "            d = rename_all_repairs_in_line_with_index(list(d))\n",
    "            gold_data[dialogue] = (a,b,c,d)\n",
    "\n",
    "        #the below does just the final output evaluation, assuming a final output file, faster\n",
    "        hyp_file = hyp_dir + '/' + system + \"/\" + \"swbd_disf_{0}{1}_data_output_final.text\".format(division,\n",
    "                                                                                                        partial)\n",
    "        word = True  # world-level analyses\n",
    "        error = True # get an error analysis\n",
    "        results,speaker_rate_dict,error_analysis = final_output_disfluency_eval_from_file(\n",
    "                                                        hyp_file,\n",
    "                                                        gold_data,\n",
    "                                                        utt_eval=\"disf only\" not in system_name,\n",
    "                                                        error_analysis=error,\n",
    "                                                        word=word,\n",
    "                                                        interval=False,\n",
    "                                                        outputfilename=None\n",
    "                                                    )\n",
    "        #the below does incremental and final output in one, also outputting the final outputs\n",
    "        #derivable from the incremental output, takes quite a while\n",
    "        for k,v in results.items():\n",
    "            print k,v\n",
    "        all_results[division + \"_\" + system] = deepcopy(results)\n",
    "        if \"heldout\" in division:\n",
    "            # only do the error analyses on the heldout data\n",
    "            all_error_dicts[division + \"_\" + system] = deepcopy(error_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heldout_033/epoch_45', 'heldout_035/epoch_6', 'test_033/epoch_45', 'test_035/epoch_6']\n"
     ]
    }
   ],
   "source": [
    "print all_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>System (eval. method)</th>\n",
       "      <th>$F_{TTO}$ (per word)</th>\n",
       "      <th>$F_{rps}$ (per word)</th>\n",
       "      <th>$F_{e}$ (per word)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM (joint task) (transcript)</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            System (eval. method) $F_{TTO}$ (per word) $F_{rps}$ (per word)  \\\n",
       "0  LSTM (joint task) (transcript)                0.684                0.666   \n",
       "\n",
       "  $F_{e}$ (per word)  \n",
       "0              0.915  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_results = dict()\n",
    "#display_results['RNN (joint task)'] = all_results['test_033/epoch_45']\n",
    "display_results['LSTM (joint task)'] = all_results['test_035/epoch_6']\n",
    "final = convert_to_latex(display_results, eval_level=['word'], inc=False, utt_seg=False, only_include=\n",
    "                        ['f1_t/>_word', 'f1_<rps_word', 'f1_<e_word'])\n",
    "#final = final.drop(final.columns[[-2]], axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repair Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** heldout_033/epoch_45 <rps ******************************\n",
      "type ******************************\n",
      "del & (25/132) & 0.318\\\\\n",
      "rep & (734/1022) & 0.836\\\\\n",
      "sub & (560/1061) & 0.691\\\\\n",
      "0.595485327314\n",
      "len ******************************\n",
      "0 & (1/1) & 0.007\\\\\n",
      "1 & (960/1258) & 0.866\\\\\n",
      "2 & (243/531) & 0.628\\\\\n",
      "3 & (64/222) & 0.448\\\\\n",
      "4 & (31/106) & 0.453\\\\\n",
      "5 & (12/50) & 0.387\\\\\n",
      "6 & (4/25) & 0.276\\\\\n",
      "7 & (3/11) & 0.429\\\\\n",
      "8 & (1/6) & 0.286\\\\\n",
      "9 & (0/1) & 0.000\\\\\n",
      "10 & (0/1) & 0.000\\\\\n",
      "11 & (0/2) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.595485327314\n",
      "****************************** heldout_033/epoch_45 <rms ******************************\n",
      "type ******************************\n",
      "del & (0/132) & 0.000\\\\\n",
      "rep & (0/1022) & 0.000\\\\\n",
      "sub & (0/1061) & 0.000\\\\\n",
      "0.0\n",
      "len ******************************\n",
      "1 & (0/1258) & 0.000\\\\\n",
      "2 & (0/531) & 0.000\\\\\n",
      "3 & (0/223) & 0.000\\\\\n",
      "4 & (0/106) & 0.000\\\\\n",
      "5 & (0/50) & 0.000\\\\\n",
      "6 & (0/25) & 0.000\\\\\n",
      "7 & (0/11) & 0.000\\\\\n",
      "8 & (0/6) & 0.000\\\\\n",
      "9 & (0/1) & 0.000\\\\\n",
      "10 & (0/1) & 0.000\\\\\n",
      "11 & (0/2) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.0\n",
      "****************************** heldout_035/epoch_6 <rps ******************************\n",
      "type ******************************\n",
      "del & (32/132) & 0.390\\\\\n",
      "rep & (751/1022) & 0.847\\\\\n",
      "sub & (615/1061) & 0.734\\\\\n",
      "0.631151241535\n",
      "len ******************************\n",
      "0 & (1/1) & 0.006\\\\\n",
      "1 & (1013/1258) & 0.892\\\\\n",
      "2 & (255/531) & 0.649\\\\\n",
      "3 & (77/222) & 0.515\\\\\n",
      "4 & (31/106) & 0.453\\\\\n",
      "5 & (11/50) & 0.361\\\\\n",
      "6 & (5/25) & 0.333\\\\\n",
      "7 & (3/11) & 0.429\\\\\n",
      "8 & (2/6) & 0.500\\\\\n",
      "9 & (0/1) & 0.000\\\\\n",
      "10 & (0/1) & 0.000\\\\\n",
      "11 & (0/2) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.631151241535\n",
      "****************************** heldout_035/epoch_6 <rms ******************************\n",
      "type ******************************\n",
      "del & (0/132) & 0.000\\\\\n",
      "rep & (0/1022) & 0.000\\\\\n",
      "sub & (0/1061) & 0.000\\\\\n",
      "0.0\n",
      "len ******************************\n",
      "1 & (0/1258) & 0.000\\\\\n",
      "2 & (0/531) & 0.000\\\\\n",
      "3 & (0/223) & 0.000\\\\\n",
      "4 & (0/106) & 0.000\\\\\n",
      "5 & (0/50) & 0.000\\\\\n",
      "6 & (0/25) & 0.000\\\\\n",
      "7 & (0/11) & 0.000\\\\\n",
      "8 & (0/6) & 0.000\\\\\n",
      "9 & (0/1) & 0.000\\\\\n",
      "10 & (0/1) & 0.000\\\\\n",
      "11 & (0/2) & 0.000\\\\\n",
      "15 & (0/1) & 0.000\\\\\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# rps and rms errors\n",
    "#Error analyses on exact match ('rms') and getting the right repair start ('rps')\n",
    "target_tags = ['<rms', '<rps']\n",
    "\n",
    "for div,all_error in all_error_dicts.items():\n",
    "    # print div, type(all_error)\n",
    "   \n",
    "    if type(all_error) == bool: continue\n",
    "    if \"test\" in div: continue\n",
    "    #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "    for tag, errors in all_error.items():\n",
    "        if tag not in target_tags:\n",
    "            continue\n",
    "        print \"*\" * 30, div, tag, \"*\" * 30\n",
    "        # print errors\n",
    "        # continue\n",
    "        #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "        error = {\"TP\" : {}, \"FP\" : {}, \"FN\": {} }\n",
    "        for k,v in errors.items():\n",
    "            #if k == \"FP\":\n",
    "            #    continue\n",
    "            # print k, len(v)\n",
    "            typedict = defaultdict(int)\n",
    "            lendict = defaultdict(int)\n",
    "            for repair in v:\n",
    "\n",
    "                #print repair.gold_context\n",
    "                onset = \"\"\n",
    "                if tag == \"<rps\" or tag == \"<rms\":\n",
    "                    \n",
    "                    \n",
    "                    for i in range(0,len(repair.gold_context)):\n",
    "                        if repair.gold_context[i] == \"+|+\":\n",
    "                            onset = repair.gold_context[i+1]\n",
    "                            break\n",
    "\n",
    "                    word = onset.split(\"|\")[0]\n",
    "                    #if k == \"FP\":\n",
    "                    #    onset = gold_onset\n",
    "                    if \"<e\" in onset and not tag == \"<e\":\n",
    "                        typedict[\"<e\"]+=1\n",
    "                    else:\n",
    "                        if word in [\"and\",\"or\",\"but\",\"so\",\"because\",\"that\",\"although\"]:\n",
    "                            typedict[\"CC\"]+=1\n",
    "                        elif word in [\"i\",\"we\",\"they\",\"im\",\"ive\",\"he\",\"she\",\"id\"]:\n",
    "                            typedict[\"subj\"]+=1\n",
    "                        elif word in [\"you\",\"the\"] or \"$\" in word:\n",
    "                            typedict[\"proper_other\"]+=1\n",
    "                        elif word in [\"yeah\",\"no\",\"okay\",\"yes\",\"right\",\"uh-huh\"]:\n",
    "                            typedict[\"ack\"]+=1\n",
    "                        elif word in [\"it\",\"its\"]:\n",
    "                            typedict[\"it\"]+=1\n",
    "                        else:\n",
    "                            typedict[word]+=1\n",
    "                \n",
    "                if tag == \"<rps\" or tag == \"<rms\": # and not k == 'FP':\n",
    "                    if k == \"TP\" and len(repair.reparandumWords) > 8:\n",
    "                        # should not be getting any over 8 words\n",
    "                        print \"** overlength repair!\"\n",
    "                        print repair\n",
    "                    lendict[len(repair.reparandumWords) + len(repair.interregnumWords)]+=1\n",
    "                    repair_type = None\n",
    "                    if repair.type:\n",
    "                        repair_type = repair.type \n",
    "                        typedict[repair_type]+=1\n",
    "\n",
    "            error[k]['len'] = deepcopy(lendict)\n",
    "            error[k]['type'] = deepcopy(typedict)\n",
    "\n",
    "                \n",
    "        for mode in ['type', 'len']:\n",
    "            #q1. THE RECALL RATES FOR VARIOUS GOLD REPAIR TYPES\n",
    "            print mode, \"*\" * 30\n",
    "            tps = error['TP'][mode]\n",
    "            fns = error['FN'][mode]\n",
    "            fps = error['FP'][mode]\n",
    "\n",
    "            total_tps = 0\n",
    "            total_fns = 0\n",
    "            total_fps = 0\n",
    "            top_n = 50\n",
    "            all_items = list(set(tps.keys() + fns.keys()))\n",
    "            # print all_items\n",
    "            for k in sorted(all_items,  reverse=False):\n",
    "                #print k, \"*\" * 30\n",
    "                if mode == 'type' and k not in [\"rep\", \"del\", \"sub\"]:\n",
    "                    continue\n",
    "                recall_total = tps[k] + fns[k]\n",
    "                recall = 0 if tps[k] == 0 else tps[k]/recall_total\n",
    "                precision_total = tps[k] + fps[k]\n",
    "                precision = 0 if tps[k] == 0 else tps[k]/precision_total\n",
    "                fscore = 0 if precision == 0 or recall == 0 else (2 * (precision * recall))/(precision + recall)\n",
    "                # print k, ':', tps[k], \"out of\", recall_total\n",
    "                #print k, ':', tps[k], \"out of\", precision_total\n",
    "                total_tps += tps[k]\n",
    "                total_fns += fns[k]\n",
    "                total_fps += fps[k]\n",
    "                print \" & \".join([str(k), \"({0}/{1})\".format(tps[k],recall_total), \n",
    "                                  '{0:.3f}'.format(fscore)]) + \"\\\\\\\\\"\n",
    "                top_n-=1\n",
    "                if top_n <= 0:\n",
    "                    break\n",
    "            print total_tps/(total_fns + total_tps)\n",
    "\n",
    "            if False:\n",
    "                #q2. ERROR TYPE SUMMARY\n",
    "                print \"*\" * 30\n",
    "                total = sum(fns.values()+tps.values())\n",
    "\n",
    "                errormass = 0\n",
    "                errortotal = 0\n",
    "                top_n = 20\n",
    "                for k,v in sorted(tps.items(),key= lambda x: x[1],reverse=True):\n",
    "                    print k,\"&\",v,\"&\",'{0:.2f}'.format(v/total)\n",
    "                    errormass +=(v/total * 100)\n",
    "                    errortotal+=v\n",
    "                    top_n-=1\n",
    "                    if top_n <= 0: break\n",
    "                print \"total &\",errortotal,\"&\",'{0:.2f}'.format(errormass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utterance Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heldout_033/epoch_45 <type 'dict'>\n",
      "heldout_033/epoch_45 <rps\n",
      "heldout_033/epoch_45 <rms\n",
      "heldout_033/epoch_45 <e\n",
      "heldout_033/epoch_45 t/>\n",
      "\n",
      "FP 877\n",
      "left context=point|<f/><cc/> is|<f/><cc/> comfortable|<f/><cc/> upper|<f/><cc/> middle|<f/><cc/>\n",
      "right context=class|<f/><ct/> i|<f/><tc/> guess|<f/><cc/> you|<f/><cc/> might|<f/><cc/>\n",
      "gold left context=\n",
      "gold right context=\n",
      "type = None\n",
      "\n",
      "TP 2879\n",
      "left context=uh|<e/><cc/> families|<f/><cc/> of|<f/><cc/> rather|<f/><cc/> modest|<f/><cc/>\n",
      "right context=means|<f/><ct/> and|<f/><tc/> uh|<e/><cc/> our|<f/><cc/> family|<f/><cc/>\n",
      "gold left context=uh|<e/><cc/><diact type=\"sd\"/> families|<f/><cc/><diact type=\"sd\"/> of|<f/><cc/><diact type=\"sd\"/> rather|<f/><cc/><diact type=\"sd\"/> modest|<f/><cc/><diact type=\"sd\"/>\n",
      "gold right context=means|<f/><ct/><diact type=\"sd\"/> and|<f/><tc/><diact type=\"sd\"/> uh|<e/><cc/><diact type=\"sd\"/> our|<f/><cc/><diact type=\"sd\"/> family|<f/><cc/><diact type=\"sd\"/>\n",
      "type = None\n",
      "\n",
      "FN 2769\n",
      "left context=\n",
      "right context=turn|<f/><cc/> well|<e/><cc/> i|<f/><cc/> have|<f/><cc/> to|<f/><cc/>\n",
      "gold left context=\n",
      "gold right context=turn|<f/><ct/><diact type=\"fo_o_fw_\"_by_bc\"/> well|<e/><tc/><diact type=\"sd\"/> i|<f/><cc/><diact type=\"sd\"/> have|<f/><cc/><diact type=\"sd\"/> to|<f/><cc/><diact type=\"sd\"/>\n",
      "type = None\n",
      "heldout_035/epoch_6 <type 'dict'>\n",
      "heldout_035/epoch_6 <rps\n",
      "heldout_035/epoch_6 <rms\n",
      "heldout_035/epoch_6 <e\n",
      "heldout_035/epoch_6 t/>\n",
      "\n",
      "FP 485\n",
      "left context=point|<f/><cc/> is|<f/><cc/> comfortable|<f/><cc/> upper|<f/><cc/> middle|<f/><cc/>\n",
      "right context=class|<f/><ct/> i|<f/><tc/> guess|<f/><cc/> you|<f/><cc/> might|<f/><cc/>\n",
      "gold left context=\n",
      "gold right context=\n",
      "type = None\n",
      "\n",
      "TP 2981\n",
      "left context=i|<f/><cc/> guess|<f/><cc/> is|<f/><cc/> a|<f/><cc/> comfortable|<f/><cc/>\n",
      "right context=thing|<f/><ct/> well|<e/><tc/> we|<f/><cc/> stay|<f/><cc/> within|<f/><cc/>\n",
      "gold left context=i|<f/><cc/><diact type=\"sd\"/> guess|<f/><cc/><diact type=\"sd\"/> is|<f/><cc/><diact type=\"sd\"/> a|<f/><cc/><diact type=\"sd\"/> comfortable|<f/><cc/><diact type=\"sd\"/>\n",
      "gold right context=thing|<f/><ct/><diact type=\"sd\"/> well|<e/><tc/><diact type=\"b^m\"/> we|<f/><cc/><diact type=\"b^m\"/> stay|<f/><cc/><diact type=\"b^m\"/> within|<f/><cc/><diact type=\"b^m\"/>\n",
      "type = None\n",
      "\n",
      "FN 2667\n",
      "left context=\n",
      "right context=turn|<f/><cc/> well|<e/><cc/> i|<f/><cc/> have|<f/><cc/> to|<f/><cc/>\n",
      "gold left context=\n",
      "gold right context=turn|<f/><ct/><diact type=\"fo_o_fw_\"_by_bc\"/> well|<e/><tc/><diact type=\"sd\"/> i|<f/><cc/><diact type=\"sd\"/> have|<f/><cc/><diact type=\"sd\"/> to|<f/><cc/><diact type=\"sd\"/>\n",
      "type = None\n",
      "defaultdict(<type 'int'>, {})\n",
      "defaultdict(<type 'int'>, {})\n",
      "CC & 887 & 15.70\n",
      "subj & 381 & 6.75\n",
      "<e & 365 & 6.46\n",
      "ack & 172 & 3.05\n",
      "<rps & 171 & 3.03\n",
      " & 74 & 1.31\n",
      "proper_other & 73 & 1.29\n",
      "it & 70 & 1.24\n",
      "if & 21 & 0.37\n",
      "what & 20 & 0.35\n",
      "thats & 19 & 0.34\n",
      "my & 18 & 0.32\n",
      "a & 14 & 0.25\n",
      "when & 13 & 0.23\n",
      "do & 11 & 0.19\n",
      "theyre & 10 & 0.18\n",
      "there & 10 & 0.18\n",
      "in & 10 & 0.18\n",
      "maybe & 10 & 0.18\n",
      "at & 10 & 0.18\n",
      "theres & 9 & 0.16\n",
      "is & 9 & 0.16\n",
      "how & 7 & 0.12\n",
      "th- & 7 & 0.12\n",
      "then & 6 & 0.11\n",
      "this & 6 & 0.11\n",
      "were & 6 & 0.11\n",
      "actually & 5 & 0.09\n",
      "to & 5 & 0.09\n",
      "pretty & 5 & 0.09\n",
      "now & 5 & 0.09\n",
      "people & 5 & 0.09\n",
      "really & 5 & 0.09\n",
      "for & 5 & 0.09\n",
      "probably & 5 & 0.09\n",
      "of & 5 & 0.09\n",
      "not & 4 & 0.07\n",
      "where & 4 & 0.07\n",
      "are & 4 & 0.07\n",
      "even & 4 & 0.07\n",
      "have & 4 & 0.07\n",
      "as & 4 & 0.07\n",
      "a- & 4 & 0.07\n",
      "all & 3 & 0.05\n",
      "yep & 3 & 0.05\n",
      "lets & 3 & 0.05\n",
      "every & 3 & 0.05\n",
      "like & 3 & 0.05\n",
      "did & 3 & 0.05\n",
      "on & 3 & 0.05\n",
      "about & 3 & 0.05\n",
      "whats & 3 & 0.05\n",
      "both & 3 & 0.05\n",
      "things & 3 & 0.05\n",
      "sure & 3 & 0.05\n",
      "particularly & 2 & 0.04\n",
      "go & 2 & 0.04\n",
      "just & 2 & 0.04\n",
      "his & 2 & 0.04\n",
      "somebody & 2 & 0.04\n",
      "possibly & 2 & 0.04\n",
      "t- & 2 & 0.04\n",
      "weve & 2 & 0.04\n",
      "w- & 2 & 0.04\n",
      "our & 2 & 0.04\n",
      "heres & 2 & 0.04\n",
      "who & 2 & 0.04\n",
      "ill & 2 & 0.04\n",
      "good & 2 & 0.04\n",
      "was & 2 & 0.04\n",
      "sort & 2 & 0.04\n",
      "with & 2 & 0.04\n",
      "theyve & 2 & 0.04\n",
      "up & 2 & 0.04\n",
      "say & 2 & 0.04\n",
      "something & 2 & 0.04\n",
      "which & 2 & 0.04\n",
      "an- & 2 & 0.04\n",
      "after & 2 & 0.04\n",
      "most & 2 & 0.04\n",
      "i- & 2 & 0.04\n",
      "why & 2 & 0.04\n",
      "especially & 2 & 0.04\n",
      "sometimes & 2 & 0.04\n",
      "d- & 1 & 0.02\n",
      "pardon & 1 & 0.02\n",
      "unemployment & 1 & 0.02\n",
      "being & 1 & 0.02\n",
      "sleep & 1 & 0.02\n",
      "wed & 1 & 0.02\n",
      "late & 1 & 0.02\n",
      "big & 1 & 0.02\n",
      "television & 1 & 0.02\n",
      "ones & 1 & 0.02\n",
      "course & 1 & 0.02\n",
      "schools & 1 & 0.02\n",
      "taxes & 1 & 0.02\n",
      "leave & 1 & 0.02\n",
      "bad & 1 & 0.02\n",
      "either & 1 & 0.02\n",
      "went & 1 & 0.02\n",
      "someday & 1 & 0.02\n",
      "absolutely & 1 & 0.02\n",
      "h- & 1 & 0.02\n",
      "some & 1 & 0.02\n",
      "back & 1 & 0.02\n",
      "belie- & 1 & 0.02\n",
      "p- & 1 & 0.02\n",
      "insurance & 1 & 0.02\n",
      "ac- & 1 & 0.02\n",
      "goes & 1 & 0.02\n",
      "co- & 1 & 0.02\n",
      "be & 1 & 0.02\n",
      "youd & 1 & 0.02\n",
      "never & 1 & 0.02\n",
      "here & 1 & 0.02\n",
      "plea & 1 & 0.02\n",
      "put & 1 & 0.02\n",
      "by & 1 & 0.02\n",
      "service & 1 & 0.02\n",
      "teachers & 1 & 0.02\n",
      "against & 1 & 0.02\n",
      "pe- & 1 & 0.02\n",
      "plus & 1 & 0.02\n",
      "arent & 1 & 0.02\n",
      "youve & 1 & 0.02\n",
      "wha- & 1 & 0.02\n",
      "dont & 1 & 0.02\n",
      "doesnt & 1 & 0.02\n",
      "another & 1 & 0.02\n",
      "bu- & 1 & 0.02\n",
      "drove & 1 & 0.02\n",
      "guess & 1 & 0.02\n",
      "from & 1 & 0.02\n",
      "would & 1 & 0.02\n",
      "hes & 1 & 0.02\n",
      "hey & 1 & 0.02\n",
      "their & 1 & 0.02\n",
      "coarse & 1 & 0.02\n",
      "tell & 1 & 0.02\n",
      "selling & 1 & 0.02\n",
      "fire & 1 & 0.02\n",
      "brand & 1 & 0.02\n",
      "nobody & 1 & 0.02\n",
      "gas & 1 & 0.02\n",
      "those & 1 & 0.02\n",
      "n- & 1 & 0.02\n",
      "kind & 1 & 0.02\n",
      "unfortunately & 1 & 0.02\n",
      "these & 1 & 0.02\n",
      "shes & 1 & 0.02\n",
      "while & 1 & 0.02\n",
      "supposed & 1 & 0.02\n",
      "cab & 1 & 0.02\n",
      "didnt & 1 & 0.02\n",
      "called & 1 & 0.02\n",
      "youre & 1 & 0.02\n",
      "woman & 1 & 0.02\n",
      "any & 1 & 0.02\n",
      "again & 1 & 0.02\n",
      "theyll & 1 & 0.02\n",
      "same & 1 & 0.02\n",
      "theyd & 1 & 0.02\n",
      "other & 1 & 0.02\n",
      "higher & 1 & 0.02\n",
      "o- & 1 & 0.02\n",
      "designed & 1 & 0.02\n",
      "nothing & 1 & 0.02\n",
      "yo- & 1 & 0.02\n",
      "give & 1 & 0.02\n",
      "definitely & 1 & 0.02\n",
      "putting & 1 & 0.02\n",
      "usu- & 1 & 0.02\n",
      "first & 1 & 0.02\n",
      "once & 1 & 0.02\n",
      "total & 2667 & 47.22\n"
     ]
    }
   ],
   "source": [
    "#Error analyses\n",
    "error = {\"TP\" : {}, \"FP\" : {}, \"FN\": {} }\n",
    " \n",
    "for div,all_error in all_error_dicts.items():\n",
    "    print div, type(all_error)\n",
    "    if type(all_error) == bool: continue\n",
    "    if \"test\" in div: continue\n",
    "    #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "    for tag,errors in all_error.items():\n",
    "        print div, tag\n",
    "        #if not 'TTO only' in div or \"asr\" in div: continue\n",
    "        if not tag == \"t/>\": continue\n",
    "        for k,v in errors.items():\n",
    "    \n",
    "            print \"\"\n",
    "            #if k == \"FP\": continue\n",
    "            print k, len(v)\n",
    "            typedict = defaultdict(int)\n",
    "            lendict = defaultdict(int)\n",
    "            print v[0]\n",
    "            for repair in v:\n",
    "                #if len(repair)==0: continue\n",
    "                #print \"*\"\n",
    "                #print repair\n",
    "                #print repair.gold_context\n",
    "                onset = \"\"\n",
    "                if tag == \"<rps\":\n",
    "                    \n",
    "                    for i in rcange(0,len(repair.gold_context)):\n",
    "                        if repair.gold_context[i] == \"+|+\":\n",
    "                            onset = repair.gold_context[i+1]\n",
    "                            break\n",
    "                else:\n",
    "                    gold_onset = \"\"\n",
    "                    onset = \"\"\n",
    "                    word = \"\"\n",
    "                    if len(repair.gold_tags_right_context)>1:\n",
    "                        gold_onset = repair.gold_tags_right_context[1]\n",
    "                        onset = repair.tags_right_context[1]\n",
    "                        word = repair.words_right_context[1]\n",
    "                    #penult = repair.tags_left_context[-1]\n",
    "                    #print repair\n",
    "                    if k == \"FP\":\n",
    "                        onset = gold_onset\n",
    "                    if \"<rps\" in onset:\n",
    "                        typedict[\"<rps\"]+=1\n",
    "                    elif \"<e\" in onset:\n",
    "                        typedict[\"<e\"]+=1\n",
    "                    else:\n",
    "                        if word in [\"and\",\"or\",\"but\",\"so\",\"because\",\"that\",\"although\"]:\n",
    "                            typedict[\"CC\"]+=1\n",
    "                        elif word in [\"i\",\"we\",\"they\",\"im\",\"ive\",\"he\",\"she\",\"id\"]:\n",
    "                            typedict[\"subj\"]+=1\n",
    "                        elif word in [\"you\",\"the\"] or \"$\" in word:\n",
    "                            typedict[\"proper_other\"]+=1\n",
    "                        elif word in [\"yeah\",\"no\",\"okay\",\"yes\",\"right\",\"uh-huh\"]:\n",
    "                            typedict[\"ack\"]+=1\n",
    "                        elif word in [\"it\",\"its\"]:\n",
    "                            typedict[\"it\"]+=1\n",
    "                        else:\n",
    "                            typedict[word]+=1\n",
    "                         \n",
    "                \n",
    "                #if \"<t\" in repair.gold_context\n",
    "                #for t in [\"tt\",\"cc\",\"ct\",\"tc\"]:\n",
    "                #    if \"<\" + t + \">\" in onset:\n",
    "                #        typedict[t]+=1\n",
    "                #        if not t[0]=='t':\n",
    "                #            print repair\n",
    "                if tag == \"<rps\" and not k == 'FP':\n",
    "                    lendict[repair.type]+=1\n",
    "                error[k]['len'] = deepcopy(lendict)\n",
    "                error[k]['type'] = deepcopy(typedict)\n",
    "\n",
    "#tp = deepcopy(lendict)\n",
    "#q1. THE RECALL RATES FOR VARIOUS GOLD REPAIRS\n",
    "tp = error['TP']['len']\n",
    "print tp\n",
    "print error['FN']['len']\n",
    "for k,v in sorted(error['FN']['len'].items()):\n",
    "    print \" & \".join([k, \"({0})\".format(v + tp[k]), \n",
    "                      '{0:.1f}'.format(100 * float(tp[k])/float(v+ tp[k]))]) + \"\\\\\\\\\"\n",
    "\n",
    "tps = error['TP']['type']\n",
    "fns = error['FN']['type']\n",
    "fps = error['FP']['type']\n",
    "\n",
    "total = sum(fns.values()+tps.values())\n",
    "\n",
    "errormass = 0\n",
    "errortotal = 0\n",
    "for k,v in sorted(fns.items(),key= lambda x: x[1],reverse=True):\n",
    "    print k,\"&\",v,\"&\",'{0:.2f}'.format(v/total * 100)\n",
    "    errormass +=(v/total * 100)\n",
    "    errortotal+=v\n",
    "print \"total &\",errortotal,\"&\",'{0:.2f}'.format(errormass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #TODO for paper/future\n",
    "# - check WER for ASR results and exclude those with high ones given they might have high overlap :(\n",
    "# - need to adjust the time to detection scores based on the time it comes in from Increco?? \n",
    "#      Also for ttdetection can only use word ends unless we re-do the mapping- just needs explanation\n",
    "# - delayed accuracy based on time, or not bother? do moving window instead and plot this over time- average moving window accuracy\n",
    "# - error analysis plots\n",
    "# 036- full task with LSTM- should improve massively over 034, which also needs re-running\n",
    "# Reproduce 027 (with full training data, efficiently) and re-run with LSTM- not much time.\n",
    "#Q2 TODO the extent to which the network is memorizing- need to plug these in with the repair gold standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
